{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ParlAI Tutorial",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tkaratekinbenten/parlAI/blob/main/ParlAI_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsb-Cvf6lnVX"
      },
      "source": [
        "<img src=\"https://parl.ai/docs/_static/img/parlai.png\" width=\"700\"/>\n",
        "\n",
        "**Author**: Stephen Roller ([GitHub](https://github.com/stephenroller), [Twitter](https://twitter.com/stephenroller))\n",
        "\n",
        "\n",
        "# Welcome to the ParlAI interactive tutorial\n",
        "\n",
        "In this tutorial we will:\n",
        "\n",
        "- Chat with a neural network model!\n",
        "- Show how to use common commands in ParlAI, like inspecting data and model outputs.\n",
        "- See where to find information about many options.\n",
        "- Show how to fine-tune a pretrained model on a specific task\n",
        "- Add our own datasets to ParlAI\n",
        "- And add our own models to ParlAI\n",
        "\n",
        "We won't be running any examples of using Amazon Mechanical Turk, or connecting to Chat services, but you can check out our [docs](https://parl.ai/docs/) for more information on these areas.\n",
        "\n",
        "**Note:** *Make sure you're running this session with a GPU attached.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_bFnOWslsj9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a5cc0eb-6454-4726-9356-88a3d050a905"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jul 21 01:50:54 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMxd1KIRl9Xm"
      },
      "source": [
        "## Installing parlai\n",
        "\n",
        "We need to install ParlAI. Since we're in Google Colab, we can assume PyTorch and similar dependencies are installed already"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i93Mn_I7MOEO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d06207f-e14f-4050-fd71-e8b892f11345"
      },
      "source": [
        "!pip install -q parlai\n",
        "!pip install -q subword_nmt # extra requirement we need for this tutorial"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.4 MB 33.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 185 kB 60.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 9.1 MB 65.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 40 kB 7.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 170 kB 73.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 48 kB 6.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 73 kB 2.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 124 kB 78.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 68 kB 7.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 208 kB 59.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.7 MB 63.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 131 kB 72.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 123 kB 78.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 262 kB 77.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 138 kB 58.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 68.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 547 kB 66.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.7 MB 67.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 241 kB 76.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 46 kB 4.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 112 kB 74.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 110 kB 71.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 636 kB 59.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 121 kB 75.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 90 kB 12.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 100 kB 12.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 84 kB 4.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 79 kB 10.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 86 kB 7.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 243 kB 54.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 118 kB 71.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 41 kB 753 kB/s \n",
            "\u001b[K     |████████████████████████████████| 68 kB 9.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.3 MB/s \n",
            "\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docformatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for untokenize (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtVz5dCUmFkN"
      },
      "source": [
        "# Chatting with a model\n",
        "\n",
        "Let's start by chatting interactively with a model file from our model zoo! We'll pick our \"tutorial transformer generator\" model, which is a generative transformer trained on pushshift.io Reddit. You can take a look at the [model zoo](https://parl.ai/docs/zoo.html) for a more complete list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRJGRtMKmIWV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c5f6308f-df94-4365-9bd1-24b585d1a337"
      },
      "source": [
        "# Import the Interactive script\n",
        "from parlai.scripts.interactive import Interactive\n",
        "\n",
        "# call it with particular args\n",
        "Interactive.main(\n",
        "    # the model_file is a filename path pointing to a particular model dump.\n",
        "    # Model files that begin with \"zoo:\" are special files distributed by the ParlAI team.\n",
        "    # They'll be automatically downloaded when you ask to use them.\n",
        "    model_file='zoo:tutorial_transformer_generator/model'\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "01:52:29 | building data: /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/tutorial_transformer_generator_v1.tar.gz\n",
            "01:52:29 | Downloading http://parl.ai/downloads/_models/tutorial_transformer_generator/tutorial_transformer_generator_v1.tar.gz to /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/tutorial_transformer_generator_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading tutorial_transformer_generator_v1.tar.gz: 100%|██████████| 1.12G/1.12G [00:57<00:00, 19.5MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01:53:46 | \u001b[33mOverriding opt[\"model_file\"] to /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model (previously: /checkpoint/roller/20190909/cleanreddit/585/model)\u001b[0m\n",
            "01:53:46 | \u001b[33mLoading model with `--beam-block-full-context false`\u001b[0m\n",
            "01:53:46 | Using CUDA\n",
            "01:53:46 | loading dictionary from /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "01:53:46 | num words = 54944\n",
            "01:53:46 | TransformerGenerator: full interactive mode on.\n",
            "01:53:47 | \u001b[33mDEPRECATED: XLM should only be used for backwards compatibility, as it involves a less-stable layernorm operation.\u001b[0m\n",
            "01:54:00 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "01:54:00 | Loading existing model params from /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "01:54:01 | Opt:\n",
            "01:54:01 |     activation: gelu\n",
            "01:54:01 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "01:54:01 |     adam_eps: 1e-06\n",
            "01:54:01 |     add_p1_after_newln: False\n",
            "01:54:01 |     aggregate_micro: False\n",
            "01:54:01 |     allow_missing_init_opts: False\n",
            "01:54:01 |     attention_dropout: 0.0\n",
            "01:54:01 |     batch_length_range: 5\n",
            "01:54:01 |     batch_sort_cache_type: pop\n",
            "01:54:01 |     batch_sort_field: text\n",
            "01:54:01 |     batchsize: 48\n",
            "01:54:01 |     beam_block_full_context: False\n",
            "01:54:01 |     beam_block_list_filename: None\n",
            "01:54:01 |     beam_block_ngram: 3\n",
            "01:54:01 |     beam_context_block_ngram: 3\n",
            "01:54:01 |     beam_delay: 30\n",
            "01:54:01 |     beam_length_penalty: 0.65\n",
            "01:54:01 |     beam_min_length: 10\n",
            "01:54:01 |     beam_min_n_best: 3\n",
            "01:54:01 |     beam_size: 8\n",
            "01:54:01 |     betas: '[0.9, 0.98]'\n",
            "01:54:01 |     bpe_add_prefix_space: None\n",
            "01:54:01 |     bpe_debug: False\n",
            "01:54:01 |     bpe_dropout: None\n",
            "01:54:01 |     bpe_merge: None\n",
            "01:54:01 |     bpe_vocab: None\n",
            "01:54:01 |     compute_tokenized_bleu: False\n",
            "01:54:01 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "01:54:01 |     datatype: train:stream\n",
            "01:54:01 |     delimiter: '\\n'\n",
            "01:54:01 |     dict_build_first: True\n",
            "01:54:01 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "01:54:01 |     dict_endtoken: __end__\n",
            "01:54:01 |     dict_file: /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "01:54:01 |     dict_include_test: False\n",
            "01:54:01 |     dict_include_valid: False\n",
            "01:54:01 |     dict_initpath: None\n",
            "01:54:01 |     dict_language: english\n",
            "01:54:01 |     dict_loaded: True\n",
            "01:54:01 |     dict_lower: True\n",
            "01:54:01 |     dict_max_ngram_size: -1\n",
            "01:54:01 |     dict_maxexs: -1\n",
            "01:54:01 |     dict_maxtokens: -1\n",
            "01:54:01 |     dict_minfreq: 0\n",
            "01:54:01 |     dict_nulltoken: __null__\n",
            "01:54:01 |     dict_starttoken: __start__\n",
            "01:54:01 |     dict_textfields: text,labels\n",
            "01:54:01 |     dict_tokenizer: bpe\n",
            "01:54:01 |     dict_unktoken: __unk__\n",
            "01:54:01 |     display_add_fields: \n",
            "01:54:01 |     display_examples: False\n",
            "01:54:01 |     display_prettify: False\n",
            "01:54:01 |     distributed_world_size: 64\n",
            "01:54:01 |     download_path: None\n",
            "01:54:01 |     dropout: 0.1\n",
            "01:54:01 |     dynamic_batching: None\n",
            "01:54:01 |     embedding_projection: random\n",
            "01:54:01 |     embedding_size: 512\n",
            "01:54:01 |     embedding_type: random\n",
            "01:54:01 |     embeddings_scale: True\n",
            "01:54:01 |     eval_batchsize: None\n",
            "01:54:01 |     evaltask: None\n",
            "01:54:01 |     ffn_size: 2048\n",
            "01:54:01 |     force_fp16_tokens: True\n",
            "01:54:01 |     fp16: True\n",
            "01:54:01 |     fp16_impl: safe\n",
            "01:54:01 |     gpu: 0\n",
            "01:54:01 |     gradient_clip: 10.0\n",
            "01:54:01 |     hide_labels: False\n",
            "01:54:01 |     history_add_global_end_token: None\n",
            "01:54:01 |     history_reversed: False\n",
            "01:54:01 |     history_size: -1\n",
            "01:54:01 |     image_cropsize: 224\n",
            "01:54:01 |     image_mode: raw\n",
            "01:54:01 |     image_size: 256\n",
            "01:54:01 |     inference: beam\n",
            "01:54:01 |     init_model: None\n",
            "01:54:01 |     init_opt: None\n",
            "01:54:01 |     interactive_mode: True\n",
            "01:54:01 |     interactive_task: True\n",
            "01:54:01 |     invsqrt_lr_decay_gamma: -1\n",
            "01:54:01 |     is_debug: False\n",
            "01:54:01 |     label_truncate: 128\n",
            "01:54:01 |     learn_positional_embeddings: True\n",
            "01:54:01 |     learningrate: 0.0005\n",
            "01:54:01 |     local_human_candidates_file: None\n",
            "01:54:01 |     log_every_n_secs: 30.0\n",
            "01:54:01 |     log_keep_fields: all\n",
            "01:54:01 |     loglevel: info\n",
            "01:54:01 |     lr_scheduler: invsqrt\n",
            "01:54:01 |     lr_scheduler_decay: 0.5\n",
            "01:54:01 |     lr_scheduler_patience: 3\n",
            "01:54:01 |     max_train_time: -1\n",
            "01:54:01 |     metrics: default\n",
            "01:54:01 |     model: transformer/generator\n",
            "01:54:01 |     model_file: /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "01:54:01 |     model_parallel: False\n",
            "01:54:01 |     momentum: 0\n",
            "01:54:01 |     multitask_weights: [1]\n",
            "01:54:01 |     n_decoder_layers: -1\n",
            "01:54:01 |     n_encoder_layers: -1\n",
            "01:54:01 |     n_heads: 16\n",
            "01:54:01 |     n_layers: 8\n",
            "01:54:01 |     n_positions: 512\n",
            "01:54:01 |     n_segments: 0\n",
            "01:54:01 |     nesterov: True\n",
            "01:54:01 |     no_cuda: False\n",
            "01:54:01 |     num_epochs: 5.0\n",
            "01:54:01 |     numthreads: 1\n",
            "01:54:01 |     numworkers: 4\n",
            "01:54:01 |     nus: [0.7]\n",
            "01:54:01 |     optimizer: fused_adam\n",
            "01:54:01 |     outfile: \n",
            "01:54:01 |     output_scaling: 1.0\n",
            "01:54:01 |     override: \"{'model_file': '/usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model'}\"\n",
            "01:54:01 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "01:54:01 |     person_tokens: False\n",
            "01:54:01 |     port: 61337\n",
            "01:54:01 |     pytorch_context_length: -1\n",
            "01:54:01 |     pytorch_datapath: None\n",
            "01:54:01 |     pytorch_include_labels: True\n",
            "01:54:01 |     pytorch_preprocess: False\n",
            "01:54:01 |     pytorch_teacher_batch_sort: False\n",
            "01:54:01 |     pytorch_teacher_dataset: None\n",
            "01:54:01 |     pytorch_teacher_task: None\n",
            "01:54:01 |     rank_candidates: False\n",
            "01:54:01 |     relu_dropout: 0.0\n",
            "01:54:01 |     save_after_valid: True\n",
            "01:54:01 |     save_every_n_secs: -1\n",
            "01:54:01 |     save_format: conversations\n",
            "01:54:01 |     share_word_embeddings: True\n",
            "01:54:01 |     short_final_eval: True\n",
            "01:54:01 |     show_advanced_args: False\n",
            "01:54:01 |     shuffle: False\n",
            "01:54:01 |     single_turn: False\n",
            "01:54:01 |     skip_generation: False\n",
            "01:54:01 |     special_tok_lst: None\n",
            "01:54:01 |     split_lines: False\n",
            "01:54:01 |     starttime: Jul21_01-53\n",
            "01:54:01 |     task: internal:new_reddit:presorted\n",
            "01:54:01 |     temperature: 1.0\n",
            "01:54:01 |     tensorboard_log: False\n",
            "01:54:01 |     text_truncate: 512\n",
            "01:54:01 |     topk: 10\n",
            "01:54:01 |     topp: 0.9\n",
            "01:54:01 |     truncate: -1\n",
            "01:54:01 |     update_freq: 1\n",
            "01:54:01 |     use_reply: label\n",
            "01:54:01 |     validation_cutoff: 1.0\n",
            "01:54:01 |     validation_every_n_epochs: -1\n",
            "01:54:01 |     validation_every_n_secs: 1800.0\n",
            "01:54:01 |     validation_max_exs: 9920\n",
            "01:54:01 |     validation_metric: ppl\n",
            "01:54:01 |     validation_metric_mode: min\n",
            "01:54:01 |     validation_patience: 0\n",
            "01:54:01 |     validation_share_agent: False\n",
            "01:54:01 |     variant: xlm\n",
            "01:54:01 |     verbose: False\n",
            "01:54:01 |     warmup_rate: 0.0001\n",
            "01:54:01 |     warmup_updates: 20000\n",
            "01:54:01 |     weight_decay: 0.01\n",
            "\u001b[1;31mEnter [DONE] if you want to end the episode, [EXIT] to quit.\u001b[0;0m\n",
            "01:54:01 | creating task(s): interactive\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m which model are you?\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mi ' m not a model , but i am a model .\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m you contradict yourself\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mi don ' t know what you ' re talking about .\u001b[0;0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-4afa600fbda9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Model files that begin with \"zoo:\" are special files distributed by the ParlAI team.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# They'll be automatically downloaded when you ask to use them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmodel_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'zoo:tutorial_transformer_generator/model'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/script.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/script.py\u001b[0m in \u001b[0;36m_run_kwargs\u001b[0;34m(cls, kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_from_parser_and_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/script.py\u001b[0m in \u001b[0;36m_run_from_parser_and_opt\u001b[0;34m(cls, opt, parser)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mscript\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/scripts/interactive.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/scripts/interactive.py\u001b[0m in \u001b[0;36minteractive\u001b[0;34m(opt)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m# Show some example dialogs:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparley\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_total_parleys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# chat was reset with [DONE], [EXIT] or EOF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/tasks/interactive/worlds.py\u001b[0m in \u001b[0;36mparley\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_act\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/agents/local_human/local_human.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetID\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mreply_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter Your Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinished\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hfUEgovmWay"
      },
      "source": [
        "The same on the command line:\n",
        "\n",
        "\n",
        "```bash\n",
        "python -m parlai.scripts.interactive --model-file zoo:tutorial_transformer_generator/model\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_hGrZGGmaWF"
      },
      "source": [
        "# Taking a look at some data\n",
        "\n",
        "We can look at look into a specific dataset. Let's look into the \"empathetic dialogues\" dataset, which aims to teach models how to respond with text expressing the appropriate emotion. We have over existing 80 datasets in ParlAI. You can take a full look in our [task list](https://parl.ai/docs/tasks.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqckSXqlmWuT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85c50b50-0336-40c0-83c7-29d373b6a23b"
      },
      "source": [
        "# The display_data script is used to show the contents of a particular task.\n",
        "# By default, we show the train\n",
        "from parlai.scripts.display_data import DisplayData\n",
        "DisplayData.main(task='empathetic_dialogues', num_examples=5)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02:08:12 | Opt:\n",
            "02:08:12 |     allow_missing_init_opts: False\n",
            "02:08:12 |     batchsize: 1\n",
            "02:08:12 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "02:08:12 |     datatype: train:ordered\n",
            "02:08:12 |     dict_class: None\n",
            "02:08:12 |     display_add_fields: \n",
            "02:08:12 |     download_path: None\n",
            "02:08:12 |     dynamic_batching: None\n",
            "02:08:12 |     hide_labels: False\n",
            "02:08:12 |     ignore_agent_reply: True\n",
            "02:08:12 |     image_cropsize: 224\n",
            "02:08:12 |     image_mode: raw\n",
            "02:08:12 |     image_size: 256\n",
            "02:08:12 |     init_model: None\n",
            "02:08:12 |     init_opt: None\n",
            "02:08:12 |     is_debug: False\n",
            "02:08:12 |     loglevel: info\n",
            "02:08:12 |     max_display_len: 1000\n",
            "02:08:12 |     model: None\n",
            "02:08:12 |     model_file: None\n",
            "02:08:12 |     multitask_weights: [1]\n",
            "02:08:12 |     mutators: None\n",
            "02:08:12 |     num_examples: 5\n",
            "02:08:12 |     override: \"{'task': 'empathetic_dialogues', 'num_examples': 5}\"\n",
            "02:08:12 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "02:08:12 |     remove_political_convos: False\n",
            "02:08:12 |     starttime: Jul21_02-08\n",
            "02:08:12 |     task: empathetic_dialogues\n",
            "02:08:12 |     train_experiencer_only: False\n",
            "02:08:12 |     verbose: False\n",
            "02:08:12 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? False, datatype: train:ordered\n",
            "[building data: /usr/local/lib/python3.7/dist-packages/data/empatheticdialogues]\n",
            "02:08:12 | Downloading http://parl.ai/downloads/empatheticdialogues/empatheticdialogues.tar.gz to /usr/local/lib/python3.7/dist-packages/data/empatheticdialogues/empatheticdialogues.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading empatheticdialogues.tar.gz: 100%|██████████| 28.0M/28.0M [00:02<00:00, 10.7MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0mI remember going to see the fireworks with my best friend. It was the first time we ever spent time alone together. Although there was a lot of people, we felt like the only people in the world.\u001b[0;0m\n",
            "   \u001b[1;94mWas this a friend you were in love with, or just a best friend?\u001b[0;0m\n",
            "\u001b[0mThis was a best friend. I miss her.\u001b[0;0m\n",
            "   \u001b[1;94mWhere has she gone?\u001b[0;0m\n",
            "\u001b[0mWe no longer talk.\u001b[0;0m\n",
            "   \u001b[1;94mOh was this something that happened because of an argument?\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0mWas this a friend you were in love with, or just a best friend?\u001b[0;0m\n",
            "   \u001b[1;94mThis was a best friend. I miss her.\u001b[0;0m\n",
            "\u001b[0mWhere has she gone?\u001b[0;0m\n",
            "   \u001b[1;94mWe no longer talk.\u001b[0;0m\n",
            "02:08:17 | loaded 39057 episodes with a total of 64636 examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9C6oHq87zGx"
      },
      "source": [
        "The black, unindented text is the _prompt_, while the blue text is the _label_. That is, the label is what we will be training the model to mimic.\n",
        "\n",
        "We can also ask to see fewer examples, and get them from the validation set instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGNSBetWmfGF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd2537c0-5e96-43ad-d17f-daf6ebff8874"
      },
      "source": [
        "# we can instead ask to see fewer examples, and get them from the valid set.\n",
        "DisplayData.main(task='empathetic_dialogues', num_examples=3, datatype='valid')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19:24:27 | Opt:\n",
            "19:24:27 |     allow_missing_init_opts: False\n",
            "19:24:27 |     batchsize: 1\n",
            "19:24:27 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "19:24:27 |     datatype: valid\n",
            "19:24:27 |     dict_class: None\n",
            "19:24:27 |     display_add_fields: \n",
            "19:24:27 |     download_path: None\n",
            "19:24:27 |     dynamic_batching: None\n",
            "19:24:27 |     hide_labels: False\n",
            "19:24:27 |     ignore_agent_reply: True\n",
            "19:24:27 |     image_cropsize: 224\n",
            "19:24:27 |     image_mode: raw\n",
            "19:24:27 |     image_size: 256\n",
            "19:24:27 |     init_model: None\n",
            "19:24:27 |     init_opt: None\n",
            "19:24:27 |     is_debug: False\n",
            "19:24:27 |     loglevel: info\n",
            "19:24:27 |     max_display_len: 1000\n",
            "19:24:27 |     model: None\n",
            "19:24:27 |     model_file: None\n",
            "19:24:27 |     multitask_weights: [1]\n",
            "19:24:27 |     mutators: None\n",
            "19:24:27 |     num_examples: 3\n",
            "19:24:27 |     override: \"{'task': 'empathetic_dialogues', 'num_examples': 3, 'datatype': 'valid'}\"\n",
            "19:24:27 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "19:24:27 |     remove_political_convos: False\n",
            "19:24:27 |     starttime: Jul20_19-24\n",
            "19:24:27 |     task: empathetic_dialogues\n",
            "19:24:27 |     train_experiencer_only: False\n",
            "19:24:27 |     verbose: False\n",
            "19:24:27 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0mToday,as i was leaving for work in the morning,i had a tire burst in the middle of a busy road. That scared the hell out of me!\u001b[0;0m\n",
            "   \u001b[1;94mAre you fine now?\u001b[0;0m\n",
            "\u001b[0mYeah,i'm doing alright now, but with minor injuries.\u001b[0;0m\n",
            "   \u001b[1;94mCool :) Is your car damaged a lot?\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0mA few weeks ago, I was walking through my hallway, minding my own business, when all of a sudden a hand reached out from under a table and grabbed my ankle. I was so suprised. I thought i was got. Turns out, it was my son. \u001b[0;0m\n",
            "   \u001b[1;94mThat's funny, hope he didn't give you a heart attack.\u001b[0;0m\n",
            "19:24:28 | loaded 2769 episodes with a total of 5738 examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVSrgRrEmdS-"
      },
      "source": [
        "On the command line:\n",
        "```bash\n",
        "python -m parlai.scripts.display_data --task empathetic_dialogues\n",
        "```\n",
        "or a bit shorter\n",
        "```\n",
        "python -m parlai.scripts.display_data -t empathetic_dialogues\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_M8Zr86n2_G"
      },
      "source": [
        "# Training a model\n",
        "\n",
        "Well it's one thing looking at data, but what if we want to train our own model (from scratch)? Let's train a very simple seq2seq LSTM with attention, to respond to empathetic dialogues.\n",
        "\n",
        "To get some extra performance, we'll initialize using GloVe embeddings, but we will cap the training time to 2 minutes for this tutorial. It won't perform very well, but that's okay."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBhVQycSn2q_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be7128d6-d9db-415f-f35e-a22c044db753"
      },
      "source": [
        "# we'll save it in the \"from_scratch_model\" directory\n",
        "!rm -rf from_scratch_model\n",
        "!mkdir -p from_scratch_model\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "TrainModel.main(\n",
        "    # we MUST provide a filename\n",
        "    model_file='from_scratch_model/model',\n",
        "    # train on empathetic dialogues\n",
        "    task='empathetic_dialogues',\n",
        "    # limit training time to 2 minutes, and a batchsize of 16\n",
        "    max_train_time=2 * 60,\n",
        "    batchsize=16,\n",
        "    \n",
        "    # we specify the model type as seq2seq\n",
        "    model='seq2seq',\n",
        "    # some hyperparamter choices. We'll use attention. We could use pretrained\n",
        "    # embeddings too, with embedding_type='fasttext', but they take a long\n",
        "    # time to download.\n",
        "    attention='dot',\n",
        "    # tie the word embeddings of the encoder/decoder/softmax.\n",
        "    lookuptable='all',\n",
        "    # truncate text and labels at 64 tokens, for memory and time savings\n",
        "    truncate=64,\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02:12:31 | building dictionary first...\n",
            "02:12:31 | Opt:\n",
            "02:12:31 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "02:12:31 |     adam_eps: 1e-08\n",
            "02:12:31 |     add_p1_after_newln: False\n",
            "02:12:31 |     aggregate_micro: False\n",
            "02:12:31 |     allow_missing_init_opts: False\n",
            "02:12:31 |     attention: dot\n",
            "02:12:31 |     attention_length: 48\n",
            "02:12:31 |     attention_time: post\n",
            "02:12:31 |     batchsize: 1\n",
            "02:12:31 |     beam_block_full_context: True\n",
            "02:12:31 |     beam_block_list_filename: None\n",
            "02:12:31 |     beam_block_ngram: -1\n",
            "02:12:31 |     beam_context_block_ngram: -1\n",
            "02:12:31 |     beam_delay: 30\n",
            "02:12:31 |     beam_length_penalty: 0.65\n",
            "02:12:31 |     beam_min_length: 1\n",
            "02:12:31 |     beam_size: 1\n",
            "02:12:31 |     betas: '(0.9, 0.999)'\n",
            "02:12:31 |     bidirectional: False\n",
            "02:12:31 |     bpe_add_prefix_space: None\n",
            "02:12:31 |     bpe_debug: False\n",
            "02:12:31 |     bpe_dropout: None\n",
            "02:12:31 |     bpe_merge: None\n",
            "02:12:31 |     bpe_vocab: None\n",
            "02:12:31 |     compute_tokenized_bleu: False\n",
            "02:12:31 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "02:12:31 |     datatype: train\n",
            "02:12:31 |     decoder: same\n",
            "02:12:31 |     delimiter: '\\n'\n",
            "02:12:31 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "02:12:31 |     dict_endtoken: __end__\n",
            "02:12:31 |     dict_file: from_scratch_model/model.dict\n",
            "02:12:31 |     dict_include_test: False\n",
            "02:12:31 |     dict_include_valid: False\n",
            "02:12:31 |     dict_initpath: None\n",
            "02:12:31 |     dict_language: english\n",
            "02:12:31 |     dict_loaded: False\n",
            "02:12:31 |     dict_lower: False\n",
            "02:12:31 |     dict_max_ngram_size: -1\n",
            "02:12:31 |     dict_maxexs: -1\n",
            "02:12:31 |     dict_maxtokens: -1\n",
            "02:12:31 |     dict_minfreq: 0\n",
            "02:12:31 |     dict_nulltoken: __null__\n",
            "02:12:31 |     dict_starttoken: __start__\n",
            "02:12:31 |     dict_textfields: text,labels\n",
            "02:12:31 |     dict_tokenizer: re\n",
            "02:12:31 |     dict_unktoken: __unk__\n",
            "02:12:31 |     display_examples: False\n",
            "02:12:31 |     download_path: None\n",
            "02:12:31 |     dropout: 0.1\n",
            "02:12:31 |     dynamic_batching: None\n",
            "02:12:31 |     embedding_projection: random\n",
            "02:12:31 |     embedding_type: random\n",
            "02:12:31 |     embeddingsize: 128\n",
            "02:12:31 |     eval_batchsize: None\n",
            "02:12:31 |     eval_dynamic_batching: None\n",
            "02:12:31 |     evaltask: None\n",
            "02:12:31 |     force_fp16_tokens: False\n",
            "02:12:31 |     fp16: False\n",
            "02:12:31 |     fp16_impl: safe\n",
            "02:12:31 |     gpu: -1\n",
            "02:12:31 |     gradient_clip: 0.1\n",
            "02:12:31 |     hiddensize: 128\n",
            "02:12:31 |     hide_labels: False\n",
            "02:12:31 |     history_add_global_end_token: None\n",
            "02:12:31 |     history_reversed: False\n",
            "02:12:31 |     history_size: -1\n",
            "02:12:31 |     image_cropsize: 224\n",
            "02:12:31 |     image_mode: no_image_model\n",
            "02:12:31 |     image_size: 256\n",
            "02:12:31 |     inference: greedy\n",
            "02:12:31 |     init_model: None\n",
            "02:12:31 |     init_opt: None\n",
            "02:12:31 |     input_dropout: 0.0\n",
            "02:12:31 |     interactive_mode: False\n",
            "02:12:31 |     invsqrt_lr_decay_gamma: -1\n",
            "02:12:31 |     is_debug: False\n",
            "02:12:31 |     label_truncate: None\n",
            "02:12:31 |     learningrate: 1\n",
            "02:12:31 |     load_from_checkpoint: True\n",
            "02:12:31 |     log_every_n_secs: -1\n",
            "02:12:31 |     log_every_n_steps: 50\n",
            "02:12:31 |     loglevel: info\n",
            "02:12:31 |     lookuptable: all\n",
            "02:12:31 |     lr_scheduler: reduceonplateau\n",
            "02:12:31 |     lr_scheduler_decay: 0.5\n",
            "02:12:31 |     lr_scheduler_patience: 3\n",
            "02:12:31 |     max_train_steps: -1\n",
            "02:12:31 |     max_train_time: 120.0\n",
            "02:12:31 |     metrics: default\n",
            "02:12:31 |     model: seq2seq\n",
            "02:12:31 |     model_file: from_scratch_model/model\n",
            "02:12:31 |     momentum: 0\n",
            "02:12:31 |     multitask_weights: [1]\n",
            "02:12:31 |     mutators: None\n",
            "02:12:31 |     nesterov: True\n",
            "02:12:31 |     no_cuda: False\n",
            "02:12:31 |     num_epochs: -1\n",
            "02:12:31 |     num_workers: 0\n",
            "02:12:31 |     numlayers: 2\n",
            "02:12:31 |     numsoftmax: 1\n",
            "02:12:31 |     nus: (0.7,)\n",
            "02:12:31 |     optimizer: sgd\n",
            "02:12:31 |     override: \"{'model_file': 'from_scratch_model/model', 'task': 'empathetic_dialogues', 'max_train_time': 120.0, 'batchsize': 16, 'model': 'seq2seq', 'attention': 'dot', 'lookuptable': 'all', 'truncate': 64}\"\n",
            "02:12:31 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "02:12:31 |     person_tokens: False\n",
            "02:12:31 |     rank_candidates: False\n",
            "02:12:31 |     remove_political_convos: False\n",
            "02:12:31 |     rnn_class: lstm\n",
            "02:12:31 |     save_after_valid: False\n",
            "02:12:31 |     save_every_n_secs: -1\n",
            "02:12:31 |     short_final_eval: False\n",
            "02:12:31 |     skip_generation: False\n",
            "02:12:31 |     special_tok_lst: None\n",
            "02:12:31 |     split_lines: False\n",
            "02:12:31 |     starttime: Jul21_02-12\n",
            "02:12:31 |     task: empathetic_dialogues\n",
            "02:12:31 |     temperature: 1.0\n",
            "02:12:31 |     tensorboard_log: False\n",
            "02:12:31 |     tensorboard_logdir: None\n",
            "02:12:31 |     text_truncate: None\n",
            "02:12:31 |     topk: 10\n",
            "02:12:31 |     topp: 0.9\n",
            "02:12:31 |     train_experiencer_only: False\n",
            "02:12:31 |     truncate: 64\n",
            "02:12:31 |     update_freq: 1\n",
            "02:12:31 |     use_reply: label\n",
            "02:12:31 |     validation_cutoff: 1.0\n",
            "02:12:31 |     validation_every_n_epochs: -1\n",
            "02:12:31 |     validation_every_n_secs: -1\n",
            "02:12:31 |     validation_every_n_steps: -1\n",
            "02:12:31 |     validation_max_exs: -1\n",
            "02:12:31 |     validation_metric: accuracy\n",
            "02:12:31 |     validation_metric_mode: None\n",
            "02:12:31 |     validation_patience: 10\n",
            "02:12:31 |     validation_share_agent: False\n",
            "02:12:31 |     verbose: False\n",
            "02:12:31 |     wandb_entity: None\n",
            "02:12:31 |     wandb_log: False\n",
            "02:12:31 |     wandb_name: None\n",
            "02:12:31 |     wandb_project: None\n",
            "02:12:31 |     warmup_rate: 0.0001\n",
            "02:12:31 |     warmup_updates: -1\n",
            "02:12:31 |     weight_decay: None\n",
            "02:12:31 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? False, datatype: train:ordered:stream\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Building dictionary: 100%|██████████| 64.6k/64.6k [00:02<00:00, 22.2kex/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "02:12:35 | Saving dictionary to from_scratch_model/model.dict\n",
            "02:12:35 | dictionary built with 22419 tokens in 0.0s\n",
            "02:12:35 | No model with opt yet at: from_scratch_model/model(.opt)\n",
            "02:12:35 | Using CUDA\n",
            "02:12:35 | loading dictionary from from_scratch_model/model.dict\n",
            "02:12:35 | num words = 22419\n",
            "02:12:35 | Total parameters: 3,453,203 (3,453,203 trainable)\n",
            "02:12:35 | Opt:\n",
            "02:12:35 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "02:12:35 |     adam_eps: 1e-08\n",
            "02:12:35 |     add_p1_after_newln: False\n",
            "02:12:35 |     aggregate_micro: False\n",
            "02:12:35 |     allow_missing_init_opts: False\n",
            "02:12:35 |     attention: dot\n",
            "02:12:35 |     attention_length: 48\n",
            "02:12:35 |     attention_time: post\n",
            "02:12:35 |     batchsize: 16\n",
            "02:12:35 |     beam_block_full_context: True\n",
            "02:12:35 |     beam_block_list_filename: None\n",
            "02:12:35 |     beam_block_ngram: -1\n",
            "02:12:35 |     beam_context_block_ngram: -1\n",
            "02:12:35 |     beam_delay: 30\n",
            "02:12:35 |     beam_length_penalty: 0.65\n",
            "02:12:35 |     beam_min_length: 1\n",
            "02:12:35 |     beam_size: 1\n",
            "02:12:35 |     betas: '(0.9, 0.999)'\n",
            "02:12:35 |     bidirectional: False\n",
            "02:12:35 |     bpe_add_prefix_space: None\n",
            "02:12:35 |     bpe_debug: False\n",
            "02:12:35 |     bpe_dropout: None\n",
            "02:12:35 |     bpe_merge: None\n",
            "02:12:35 |     bpe_vocab: None\n",
            "02:12:35 |     compute_tokenized_bleu: False\n",
            "02:12:35 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "02:12:35 |     datatype: train\n",
            "02:12:35 |     decoder: same\n",
            "02:12:35 |     delimiter: '\\n'\n",
            "02:12:35 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "02:12:35 |     dict_endtoken: __end__\n",
            "02:12:35 |     dict_file: from_scratch_model/model.dict\n",
            "02:12:35 |     dict_include_test: False\n",
            "02:12:35 |     dict_include_valid: False\n",
            "02:12:35 |     dict_initpath: None\n",
            "02:12:35 |     dict_language: english\n",
            "02:12:35 |     dict_loaded: True\n",
            "02:12:35 |     dict_lower: False\n",
            "02:12:35 |     dict_max_ngram_size: -1\n",
            "02:12:35 |     dict_maxexs: -1\n",
            "02:12:35 |     dict_maxtokens: -1\n",
            "02:12:35 |     dict_minfreq: 0\n",
            "02:12:35 |     dict_nulltoken: __null__\n",
            "02:12:35 |     dict_starttoken: __start__\n",
            "02:12:35 |     dict_textfields: text,labels\n",
            "02:12:35 |     dict_tokenizer: re\n",
            "02:12:35 |     dict_unktoken: __unk__\n",
            "02:12:35 |     display_examples: False\n",
            "02:12:35 |     download_path: None\n",
            "02:12:35 |     dropout: 0.1\n",
            "02:12:35 |     dynamic_batching: None\n",
            "02:12:35 |     embedding_projection: random\n",
            "02:12:35 |     embedding_type: random\n",
            "02:12:35 |     embeddingsize: 128\n",
            "02:12:35 |     eval_batchsize: None\n",
            "02:12:35 |     eval_dynamic_batching: None\n",
            "02:12:35 |     evaltask: None\n",
            "02:12:35 |     force_fp16_tokens: False\n",
            "02:12:35 |     fp16: False\n",
            "02:12:35 |     fp16_impl: safe\n",
            "02:12:35 |     gpu: -1\n",
            "02:12:35 |     gradient_clip: 0.1\n",
            "02:12:35 |     hiddensize: 128\n",
            "02:12:35 |     hide_labels: False\n",
            "02:12:35 |     history_add_global_end_token: None\n",
            "02:12:35 |     history_reversed: False\n",
            "02:12:35 |     history_size: -1\n",
            "02:12:35 |     image_cropsize: 224\n",
            "02:12:35 |     image_mode: raw\n",
            "02:12:35 |     image_size: 256\n",
            "02:12:35 |     inference: greedy\n",
            "02:12:35 |     init_model: None\n",
            "02:12:35 |     init_opt: None\n",
            "02:12:35 |     input_dropout: 0.0\n",
            "02:12:35 |     interactive_mode: False\n",
            "02:12:35 |     invsqrt_lr_decay_gamma: -1\n",
            "02:12:35 |     is_debug: False\n",
            "02:12:35 |     label_truncate: None\n",
            "02:12:35 |     learningrate: 1\n",
            "02:12:35 |     load_from_checkpoint: True\n",
            "02:12:35 |     log_every_n_secs: -1\n",
            "02:12:35 |     log_every_n_steps: 50\n",
            "02:12:35 |     loglevel: info\n",
            "02:12:35 |     lookuptable: all\n",
            "02:12:35 |     lr_scheduler: reduceonplateau\n",
            "02:12:35 |     lr_scheduler_decay: 0.5\n",
            "02:12:35 |     lr_scheduler_patience: 3\n",
            "02:12:35 |     max_train_steps: -1\n",
            "02:12:35 |     max_train_time: 120.0\n",
            "02:12:35 |     metrics: default\n",
            "02:12:35 |     model: seq2seq\n",
            "02:12:35 |     model_file: from_scratch_model/model\n",
            "02:12:35 |     momentum: 0\n",
            "02:12:35 |     multitask_weights: [1]\n",
            "02:12:35 |     mutators: None\n",
            "02:12:35 |     nesterov: True\n",
            "02:12:35 |     no_cuda: False\n",
            "02:12:35 |     num_epochs: -1\n",
            "02:12:35 |     num_workers: 0\n",
            "02:12:35 |     numlayers: 2\n",
            "02:12:35 |     numsoftmax: 1\n",
            "02:12:35 |     nus: (0.7,)\n",
            "02:12:35 |     optimizer: sgd\n",
            "02:12:35 |     override: \"{'model_file': 'from_scratch_model/model', 'task': 'empathetic_dialogues', 'max_train_time': 120.0, 'batchsize': 16, 'model': 'seq2seq', 'attention': 'dot', 'lookuptable': 'all', 'truncate': 64}\"\n",
            "02:12:35 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "02:12:35 |     person_tokens: False\n",
            "02:12:35 |     rank_candidates: False\n",
            "02:12:35 |     remove_political_convos: False\n",
            "02:12:35 |     rnn_class: lstm\n",
            "02:12:35 |     save_after_valid: False\n",
            "02:12:35 |     save_every_n_secs: -1\n",
            "02:12:35 |     short_final_eval: False\n",
            "02:12:35 |     skip_generation: False\n",
            "02:12:35 |     special_tok_lst: None\n",
            "02:12:35 |     split_lines: False\n",
            "02:12:35 |     starttime: Jul21_02-12\n",
            "02:12:35 |     task: empathetic_dialogues\n",
            "02:12:35 |     temperature: 1.0\n",
            "02:12:35 |     tensorboard_log: False\n",
            "02:12:35 |     tensorboard_logdir: None\n",
            "02:12:35 |     text_truncate: None\n",
            "02:12:35 |     topk: 10\n",
            "02:12:35 |     topp: 0.9\n",
            "02:12:35 |     train_experiencer_only: False\n",
            "02:12:35 |     truncate: 64\n",
            "02:12:35 |     update_freq: 1\n",
            "02:12:35 |     use_reply: label\n",
            "02:12:35 |     validation_cutoff: 1.0\n",
            "02:12:35 |     validation_every_n_epochs: -1\n",
            "02:12:35 |     validation_every_n_secs: -1\n",
            "02:12:35 |     validation_every_n_steps: -1\n",
            "02:12:35 |     validation_max_exs: -1\n",
            "02:12:35 |     validation_metric: accuracy\n",
            "02:12:35 |     validation_metric_mode: None\n",
            "02:12:35 |     validation_patience: 10\n",
            "02:12:35 |     validation_share_agent: False\n",
            "02:12:35 |     verbose: False\n",
            "02:12:35 |     wandb_entity: None\n",
            "02:12:35 |     wandb_log: False\n",
            "02:12:35 |     wandb_name: None\n",
            "02:12:35 |     wandb_project: None\n",
            "02:12:35 |     warmup_rate: 0.0001\n",
            "02:12:35 |     warmup_updates: -1\n",
            "02:12:35 |     weight_decay: None\n",
            "02:12:35 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? False, datatype: train\n",
            "02:12:36 | training...\n",
            "02:12:39 | time:3s total_exs:800 total_steps:50 epochs:0.01\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   30.61     1 455.3  7064   .0925      2.152 248.1  800   1.04   .03722 15.88  9.24   1 253.5  3934  .00375     .03375 10296   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "       .06966         0                   50 708.8 10998 15.52\n",
            "\n",
            "02:12:43 | time:7s total_exs:1600 total_steps:100 epochs:0.02\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   32.77     1 474.6  7125   .1113      3.109 240.2  800  1.018   .03727 16.72 8.901   1 266.5  4001  .00625     .06125 7340   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "       .09448         0                  100 741.1 11126 15.02\n",
            "\n",
            "02:12:46 | time:10s total_exs:2400 total_steps:150 epochs:0.04\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   29.88     1 445.7  7015  .07875      2.025 251.8  800  1.105   .03721 16.06 8.567   1 256.9  4043  .00125     .00125 5258   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1124         0                  150 702.6 11058 15.74\n",
            "\n",
            "02:12:49 | time:13s total_exs:3200 total_steps:200 epochs:0.05\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "    30.8     1 457.1  7182  .09625       2.23 251.4  800  1.058   .03721  16.5 8.479   1 262.9  4130  .00375      .0675 4812   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "        .1224         0                  200  720 11312 15.71\n",
            "\n",
            "02:12:52 | time:16s total_exs:4000 total_steps:250 epochs:0.06\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   29.97     1 449.9  7152   .0900      1.855 254.4  800  1.137   .03726 16.09 8.252   1 256.7  4082   .0025     .03875 3837   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps  ups  \n",
            "        .1383         0                  250 706.6 11234 15.9\n",
            "\n",
            "02:12:55 | time:19s total_exs:4800 total_steps:300 epochs:0.07\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   30.44     1 448.5  7041  .09625      2.408 251.2  800  1.124   .03721 15.86 8.132   1 253.2  3976   .0025     .02875 3401   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1388         0                  300 701.7 11017 15.71\n",
            "\n",
            "02:12:59 | time:22s total_exs:5600 total_steps:350 epochs:0.09\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   29.75     1 451.6  7345  .07875      1.521 260.2  800  1.137   .03717 15.54 8.026   1 248.2  4037  .00125     .02375 3059   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1439         0                  350 699.8 11383 16.27\n",
            "\n",
            "02:13:02 | time:26s total_exs:6400 total_steps:400 epochs:0.10\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   31.25     1 459.9  6937   .1025      2.506 241.4  800  1.154    .0372 16.61 7.944   1   265  3997   .0050     .05375 2818   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1410         0                  400 724.9 10935 15.09\n",
            "\n",
            "02:13:05 | time:29s total_exs:7200 total_steps:450 epochs:0.11\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   30.43     1 451.4  7188   .1050      2.217 254.7  800  1.141   .03719 16.48 7.875   1 263.3  4192   .0025      .0275 2629   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1474         0                  450 714.7 11380 15.93\n",
            "\n",
            "02:13:08 | time:32s total_exs:8000 total_steps:500 epochs:0.12\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   28.86     1 434.9  6620  .08125      1.683 243.5  800   1.16   .03723 16.58 7.782   1 263.9  4017   .0050      .0825 2396   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1546         0                  500 698.8 10636 15.22\n",
            "\n",
            "02:13:12 | time:35s total_exs:8800 total_steps:550 epochs:0.14\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   32.29     1 474.4  7436   .1037      2.643 250.8  800  1.193   .03719 15.88  7.61   1 253.4  3973   .0025      .0350 2018   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1586         0                  550 727.8 11408 15.68\n",
            "\n",
            "02:13:15 | time:39s total_exs:9600 total_steps:600 epochs:0.15\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   30.24     1 451.2  7158   .0925      2.042 253.8  800  1.162   .03719 16.37 7.641   1 261.5  4149  .00125      .0250 2081   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1558         0                  600 712.7 11307 15.87\n",
            "\n",
            "02:13:18 | time:42s total_exs:10400 total_steps:650 epochs:0.16\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   30.26     1 451.2  6286   .0975      2.058 222.9  800  1.191   .03728 16.61 7.582   1 264.3  3682  .00625      .0950 1963   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1519         0                  650 715.5 9968 13.93\n",
            "\n",
            "02:13:22 | time:46s total_exs:11200 total_steps:700 epochs:0.17\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   31.31     1 453.4  6422  .09875      2.975 226.6  800  1.199   .03729  16.4 7.569   1 262.1  3712  .00375     .02125 1937   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1578         0                  700 715.5 10134 14.17\n",
            "\n",
            "02:13:26 | time:49s total_exs:12000 total_steps:750 epochs:0.19\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   31.96     1 463.5  6304   .1113      2.995 217.6  800  1.173   .03721 17.73 7.486   1 282.3  3841   .0025      .0875 1783   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1561         0                  750 745.8 10145 13.61\n",
            "\n",
            "02:13:29 | time:53s total_exs:12800 total_steps:800 epochs:0.20\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   30.55     1 453.7  6903  .09375      2.189 243.4  800  1.248   .03722 15.51 7.382   1 247.8  3771   .0025      .0175 1606   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1712         0                  800 701.5 10674 15.22\n",
            "\n",
            "02:13:32 | time:56s total_exs:13600 total_steps:850 epochs:0.21\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   30.91     1 454.8  6874   .1013      2.484 241.8  800  1.227   .03722 16.14 7.274   1 257.6  3893   .0025     .03875 1442   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1752         0                  850 712.4 10766 15.12\n",
            "\n",
            "02:13:36 | time:60s total_exs:14400 total_steps:900 epochs:0.22\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   30.23     1 446.4  6384   .0900      2.325 228.8  800  1.249   .03727 16.78 7.271   1 267.2  3821   .0050      .0800 1438   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps  ups  \n",
            "        .1689         0                  900 713.6 10205 14.3\n",
            "\n",
            "02:13:40 | time:63s total_exs:15200 total_steps:950 epochs:0.24\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   30.31     1 441.8  5733  .08625      2.695 207.6  800  1.235   .03725 17.06 7.186   1 270.4  3509   .0100      .1638 1320   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1738         0                  950 712.2 9241 12.98\n",
            "\n",
            "02:13:43 | time:67s total_exs:16000 total_steps:1000 epochs:0.25\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   29.92     1 447.2  7328   .0900      1.974 262.2  800  1.294   .03721  15.7 7.129   1 250.1  4098   .0050     .06875 1248   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1790         0                 1000 697.2 11426 16.39\n",
            "\n",
            "02:13:46 | time:70s total_exs:16800 total_steps:1050 epochs:0.26\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   32.49     1 463.4  7271   .1138       3.53   251  800  1.212   .03722 17.03 7.127   1 271.6  4262  .00375     .05125 1246   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "        .1786         0                 1050  735 11533 15.69\n",
            "\n",
            "02:13:49 | time:73s total_exs:17600 total_steps:1100 epochs:0.27\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   31.48     1 464.1  7215   .1013       2.48 248.7  800  1.257   .03722 16.48 7.132   1   263  4089  .00375      .0375 1251   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1753         0                 1100 727.1 11304 15.55\n",
            "\n",
            "02:13:52 | time:76s total_exs:18400 total_steps:1150 epochs:0.28\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "    31.6     1 460.5  6902   .1100      2.817 239.8  800  1.244   .03724 16.75 7.108   1 267.1  4003  .00375     .05375 1222   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1755         0                 1150 727.5 10905 14.99\n",
            "\n",
            "02:13:56 | time:80s total_exs:19200 total_steps:1200 epochs:0.30\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   29.63     1 448.5  6963   .0875      1.595 248.4  800  1.256   .03727 16.54 7.061   1 264.2  4102   .0025      .0275 1166   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1745         0                 1200 712.8 11065 15.53\n",
            "\n",
            "02:13:59 | time:83s total_exs:20000 total_steps:1250 epochs:0.31\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   31.02     1 451.9  6509   .0925      2.771 230.4  800  1.289   .03721 16.38 6.992   1 261.2  3761  .00625     .05375 1088   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1812         0                 1250 713.1 10270 14.41\n",
            "\n",
            "02:14:02 | time:86s total_exs:20800 total_steps:1300 epochs:0.32\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   29.92     1 448.4  6793  .09125      1.891 242.4  800  1.239    .0372 17.12 7.026   1 273.2  4138  .00375     .05125 1126   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1775         0                 1300 721.6 10931 15.15\n",
            "\n",
            "02:14:06 | time:90s total_exs:21600 total_steps:1350 epochs:0.33\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   29.58     1 444.8  6783   .0800      1.778   244  800  1.297   .03721 16.31 6.967   1 260.8  3977   .0025     .01375 1061   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1813         0                 1350 705.6 10760 15.25\n",
            "\n",
            "02:14:09 | time:93s total_exs:22400 total_steps:1400 epochs:0.35\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   29.88     1 445.4  6673  .08625      2.035 239.7  800  1.289   .03722 16.41 6.883   1   261  3910   .0025      .1025 975.6   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1918         0                 1400 706.4 10583 14.98\n",
            "\n",
            "02:14:12 | time:96s total_exs:23200 total_steps:1450 epochs:0.36\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   29.79     1   451  6960   .0800      1.601 246.9  800   1.27   .03723 16.52 6.908   1   263  4059  .00625      .0825 1000   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1883         0                 1450 714.1 11020 15.44\n",
            "\n",
            "02:14:16 | time:100s total_exs:24000 total_steps:1500 epochs:0.37\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   30.46     1   455  6877  .08625      2.029 241.8  800  1.282   .03721 16.75 6.846   1 267.4  4042   .0050      .0350 939.7   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1883         0                 1500 722.4 10919 15.12\n",
            "\n",
            "02:14:19 | time:103s total_exs:24800 total_steps:1550 epochs:0.38\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   31.73     1 456.5  6531   .1163      3.196 228.9  800  1.274   .03725 16.48 6.887   1 263.6  3771   .0025     .00625  979   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1859         0                 1550 720.1 10302 14.31\n",
            "\n",
            "02:14:23 | time:107s total_exs:25600 total_steps:1600 epochs:0.40\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   31.41     1 457.6  6611   .1013      2.804 231.1  800  1.283   .03723 16.83 6.817   1 268.7  3882   .0025     .03375 913.5   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1894         0                 1600 726.3 10493 14.45\n",
            "\n",
            "02:14:26 | time:110s total_exs:26400 total_steps:1650 epochs:0.41\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   30.15     1 443.2  6281  .08875      2.447 226.7  800  1.311   .03726 16.51 6.865   1 263.8  3738  .00125      .0200  958   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1829         0                 1650 707.1 10019 14.17\n",
            "\n",
            "02:14:30 | time:114s total_exs:27200 total_steps:1700 epochs:0.42\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   31.84     1 457.6  6519   .0975      3.243 227.9  800  1.295   .03726 16.28 6.823   1 258.5  3683  .00375      .1187 918.9   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1880         0                 1700 716.1 10202 14.25\n",
            "\n",
            "02:14:33 | time:117s total_exs:28000 total_steps:1750 epochs:0.43\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   29.51     1 436.3  6129  .09125      2.245 224.8  800  1.275    .0372 17.18 6.858   1 274.3  3854  .00375      .0325 951.8   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1850         0                 1750 710.6 9983 14.05\n",
            "\n",
            "02:14:36 | max_train_time elapsed:120.05239820480347s\n",
            "02:14:36 | Using CUDA\n",
            "02:14:36 | loading dictionary from from_scratch_model/model.dict\n",
            "02:14:36 | num words = 22419\n",
            "02:14:36 | Total parameters: 3,453,203 (3,453,203 trainable)\n",
            "02:14:36 | Loading existing model params from from_scratch_model/model\n",
            "02:14:36 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "02:14:37 | running eval: valid\n",
            "02:16:30 | eval completed in 112.94s\n",
            "02:16:30 | \u001b[1mvalid:\n",
            "    accuracy    bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs     f1  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  \\\n",
            "           0 5.964e-07 39.52 572.6  1825   .1682      3.592 50.81 5738 .06633   .01203 15.65 6.449   1 249.1 794.1 .002091   \n",
            "    ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  \n",
            "       .02457 632.3      .2159         0                 1788 821.7 2620\n",
            "\u001b[0m\n",
            "02:16:30 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: test\n",
            "02:16:30 | running eval: test\n",
            "02:18:14 | eval completed in 103.44s\n",
            "02:18:14 | \u001b[1mtest:\n",
            "    accuracy    bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs     f1  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  \\\n",
            "           0 7.615e-07 42.71 604.5  1923   .1960      4.891 50.85 5259 .06541   .01205 15.85 6.461   1 252.6 803.5 .003613   \n",
            "    ltrunclen  ppl  token_acc  token_em  total_train_updates   tpb  tps  \n",
            "       .04982  640      .2135         0                 1788 857.1 2726\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'accuracy': ExactMatchMetric(0),\n",
              "  'bleu-4': BleuMetric(5.964e-07),\n",
              "  'clen': AverageMetric(39.52),\n",
              "  'ctpb': GlobalAverageMetric(572.6),\n",
              "  'ctps': GlobalTimerMetric(1825),\n",
              "  'ctrunc': AverageMetric(0.1682),\n",
              "  'ctrunclen': AverageMetric(3.592),\n",
              "  'exps': GlobalTimerMetric(50.81),\n",
              "  'exs': SumMetric(5738),\n",
              "  'f1': F1Metric(0.06633),\n",
              "  'gpu_mem': GlobalAverageMetric(0.01203),\n",
              "  'llen': AverageMetric(15.65),\n",
              "  'loss': AverageMetric(6.449),\n",
              "  'lr': GlobalAverageMetric(1),\n",
              "  'ltpb': GlobalAverageMetric(249.1),\n",
              "  'ltps': GlobalTimerMetric(794.1),\n",
              "  'ltrunc': AverageMetric(0.002091),\n",
              "  'ltrunclen': AverageMetric(0.02457),\n",
              "  'ppl': PPLMetric(632.3),\n",
              "  'token_acc': AverageMetric(0.2159),\n",
              "  'token_em': AverageMetric(0),\n",
              "  'total_train_updates': GlobalFixedMetric(1788),\n",
              "  'tpb': GlobalAverageMetric(821.7),\n",
              "  'tps': GlobalTimerMetric(2620)},\n",
              " {'accuracy': ExactMatchMetric(0),\n",
              "  'bleu-4': BleuMetric(7.615e-07),\n",
              "  'clen': AverageMetric(42.71),\n",
              "  'ctpb': GlobalAverageMetric(604.5),\n",
              "  'ctps': GlobalTimerMetric(1923),\n",
              "  'ctrunc': AverageMetric(0.196),\n",
              "  'ctrunclen': AverageMetric(4.891),\n",
              "  'exps': GlobalTimerMetric(50.85),\n",
              "  'exs': SumMetric(5259),\n",
              "  'f1': F1Metric(0.06541),\n",
              "  'gpu_mem': GlobalAverageMetric(0.01205),\n",
              "  'llen': AverageMetric(15.85),\n",
              "  'loss': AverageMetric(6.461),\n",
              "  'lr': GlobalAverageMetric(1),\n",
              "  'ltpb': GlobalAverageMetric(252.6),\n",
              "  'ltps': GlobalTimerMetric(803.5),\n",
              "  'ltrunc': AverageMetric(0.003613),\n",
              "  'ltrunclen': AverageMetric(0.04982),\n",
              "  'ppl': PPLMetric(640),\n",
              "  'token_acc': AverageMetric(0.2135),\n",
              "  'token_em': AverageMetric(0),\n",
              "  'total_train_updates': GlobalFixedMetric(1788),\n",
              "  'tpb': GlobalAverageMetric(857.1),\n",
              "  'tps': GlobalTimerMetric(2726)})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvA77Zwkoviq"
      },
      "source": [
        "Our perplexity and F1 (word overlap) scores are pretty bad, and our BLEU-4 score is nearly 0. That's okay, we would normally want to train for well over an hour. Feel free to change the max_train_time above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QTiTn7aoxv9"
      },
      "source": [
        "## Performance is pretty bad there. Can we improve it?\n",
        "\n",
        "The easiest way to improve it is to *initialize* using a *pretrained model*, utilizing *transfer learning*. Let's use the one from the interactive session at the beginning of the chat!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2Jt9bHTn1dP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "827f0835-99a3-4602-97c0-f86db0d88644"
      },
      "source": [
        "!rm -rf from_pretrained\n",
        "!mkdir -p from_pretrained\n",
        "\n",
        "TrainModel.main(\n",
        "    # similar to before\n",
        "    task='empathetic_dialogues', \n",
        "    model='transformer/generator',\n",
        "    model_file='from_pretrained/model',\n",
        "    \n",
        "    # initialize with a pretrained model\n",
        "    init_model='zoo:tutorial_transformer_generator/model',\n",
        "    \n",
        "    # arguments we get from the pretrained model.\n",
        "    # Unfortunately, these must be looked up separately for each model.\n",
        "    n_heads=16, n_layers=8, n_positions=512, text_truncate=512,\n",
        "    label_truncate=128, ffn_size=2048, embedding_size=512,\n",
        "    activation='gelu', variant='xlm',\n",
        "    dict_lower=True, dict_tokenizer='bpe',\n",
        "    dict_file='zoo:tutorial_transformer_generator/model.dict',\n",
        "    learn_positional_embeddings=True,\n",
        "    \n",
        "    # some training arguments, specific to this fine-tuning\n",
        "    # use a small learning rate with ADAM optimizer\n",
        "    lr=1e-5, optimizer='adam',\n",
        "    warmup_updates=100,\n",
        "    # early stopping on perplexity\n",
        "    validation_metric='ppl',\n",
        "    # train at most 10 minutes, and validate every 0.25 epochs\n",
        "    max_train_time=600, validation_every_n_epochs=0.25,\n",
        "    \n",
        "    # depend on your gpu. If you have a V100, this is good\n",
        "    batchsize=12, fp16=True, fp16_impl='mem_efficient',\n",
        "    \n",
        "    # speeds up validation\n",
        "    skip_generation=True,\n",
        "    \n",
        "    # helps us cram more examples into our gpu at a time\n",
        "    dynamic_batching='full',\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02:20:20 | building dictionary first...\n",
            "02:20:20 | No model with opt yet at: from_pretrained/model(.opt)\n",
            "02:20:20 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: full,verbose: False,is_debug: False,datapath: /usr/local/lib/python3.7/dist-packages/data,eval_dynamic_batching: None,num_workers: 0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_steps: -1,load_from_checkpoint: True,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,mutators: None,train_experiencer_only: False,remove_political_convos: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,beam_block_full_context: True,beam_length_penalty: 0.65,topk: 10,topp: 0.9,beam_delay: 30,beam_block_list_filename: None,temperature: 1.0,compute_tokenized_bleu: False,interactive_mode: False,fp16_impl: mem_efficient,force_fp16_tokens: False,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,parlai_home: /usr/local/lib/python3.7/dist-packages\u001b[0m\n",
            "02:20:20 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--show-advanced-args False --task internal:new_reddit:presorted --datatype train:stream --numthreads 1 --batchsize 48 --num-epochs 5.0 --max-train-time -1 --validation-every-n-secs 1800.0 --save-after-valid True --validation-every-n-epochs -1 --validation-max-exs 9920 --short-final-eval True --validation-patience 0 --validation-metric-mode min --dict-build-first True --numworkers 4 --pytorch-preprocess False --pytorch-teacher-batch-sort False --batch-sort-cache-type pop --batch-length-range 5 --shuffle False --batch-sort-field text --pytorch-context-length -1 --pytorch-include-labels True --log-every-n-secs 30.0 --distributed-world-size 64 --port 61337 --dropout 0.1 --beam-size 8 --beam-min-n-best 3 --beam-min-length 10 --skip-generation False --inference beam --optimizer fused_adam --learningrate 0.0005 --gradient-clip 10.0 --adam-eps 1e-06 --betas 0.9,0.98 --weight-decay 0.01 --lr-scheduler invsqrt --warmup-updates 20000 --gpu 0 --beam-block-ngram 3 --beam-context-block-ngram 3\u001b[0m\n",
            "02:20:20 | Using CUDA\n",
            "02:20:20 | loading dictionary from /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "02:20:20 | num words = 54944\n",
            "02:20:21 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "02:20:21 | Loading existing model params from /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "02:20:22 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n",
            "02:20:22 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n",
            "02:20:22 | Opt:\n",
            "02:20:22 |     activation: gelu\n",
            "02:20:22 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "02:20:22 |     adam_eps: 1e-08\n",
            "02:20:22 |     add_p1_after_newln: False\n",
            "02:20:22 |     aggregate_micro: False\n",
            "02:20:22 |     allow_missing_init_opts: False\n",
            "02:20:22 |     attention_dropout: 0.0\n",
            "02:20:22 |     batchsize: 12\n",
            "02:20:22 |     beam_block_full_context: True\n",
            "02:20:22 |     beam_block_list_filename: None\n",
            "02:20:22 |     beam_block_ngram: -1\n",
            "02:20:22 |     beam_context_block_ngram: -1\n",
            "02:20:22 |     beam_delay: 30\n",
            "02:20:22 |     beam_length_penalty: 0.65\n",
            "02:20:22 |     beam_min_length: 1\n",
            "02:20:22 |     beam_size: 1\n",
            "02:20:22 |     betas: '(0.9, 0.999)'\n",
            "02:20:22 |     bpe_add_prefix_space: None\n",
            "02:20:22 |     bpe_debug: False\n",
            "02:20:22 |     bpe_dropout: None\n",
            "02:20:22 |     bpe_merge: None\n",
            "02:20:22 |     bpe_vocab: None\n",
            "02:20:22 |     compute_tokenized_bleu: False\n",
            "02:20:22 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "02:20:22 |     datatype: train\n",
            "02:20:22 |     delimiter: '\\n'\n",
            "02:20:22 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "02:20:22 |     dict_endtoken: __end__\n",
            "02:20:22 |     dict_file: /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "02:20:22 |     dict_include_test: False\n",
            "02:20:22 |     dict_include_valid: False\n",
            "02:20:22 |     dict_initpath: None\n",
            "02:20:22 |     dict_language: english\n",
            "02:20:22 |     dict_loaded: True\n",
            "02:20:22 |     dict_lower: True\n",
            "02:20:22 |     dict_max_ngram_size: -1\n",
            "02:20:22 |     dict_maxexs: -1\n",
            "02:20:22 |     dict_maxtokens: -1\n",
            "02:20:22 |     dict_minfreq: 0\n",
            "02:20:22 |     dict_nulltoken: __null__\n",
            "02:20:22 |     dict_starttoken: __start__\n",
            "02:20:22 |     dict_textfields: text,labels\n",
            "02:20:22 |     dict_tokenizer: bpe\n",
            "02:20:22 |     dict_unktoken: __unk__\n",
            "02:20:22 |     display_examples: False\n",
            "02:20:22 |     download_path: None\n",
            "02:20:22 |     dropout: 0.0\n",
            "02:20:22 |     dynamic_batching: full\n",
            "02:20:22 |     embedding_projection: random\n",
            "02:20:22 |     embedding_size: 512\n",
            "02:20:22 |     embedding_type: random\n",
            "02:20:22 |     embeddings_scale: True\n",
            "02:20:22 |     eval_batchsize: None\n",
            "02:20:22 |     eval_dynamic_batching: None\n",
            "02:20:22 |     evaltask: None\n",
            "02:20:22 |     ffn_size: 2048\n",
            "02:20:22 |     force_fp16_tokens: False\n",
            "02:20:22 |     fp16: True\n",
            "02:20:22 |     fp16_impl: mem_efficient\n",
            "02:20:22 |     gpu: -1\n",
            "02:20:22 |     gradient_clip: 0.1\n",
            "02:20:22 |     hide_labels: False\n",
            "02:20:22 |     history_add_global_end_token: None\n",
            "02:20:22 |     history_reversed: False\n",
            "02:20:22 |     history_size: -1\n",
            "02:20:22 |     image_cropsize: 224\n",
            "02:20:22 |     image_mode: raw\n",
            "02:20:22 |     image_size: 256\n",
            "02:20:22 |     inference: greedy\n",
            "02:20:22 |     init_model: /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "02:20:23 |     init_opt: None\n",
            "02:20:23 |     interactive_mode: False\n",
            "02:20:23 |     invsqrt_lr_decay_gamma: -1\n",
            "02:20:23 |     is_debug: False\n",
            "02:20:23 |     label_truncate: 128\n",
            "02:20:23 |     learn_positional_embeddings: True\n",
            "02:20:23 |     learningrate: 1e-05\n",
            "02:20:23 |     load_from_checkpoint: True\n",
            "02:20:23 |     log_every_n_secs: -1\n",
            "02:20:23 |     log_every_n_steps: 50\n",
            "02:20:23 |     loglevel: info\n",
            "02:20:23 |     lr_scheduler: reduceonplateau\n",
            "02:20:23 |     lr_scheduler_decay: 0.5\n",
            "02:20:23 |     lr_scheduler_patience: 3\n",
            "02:20:23 |     max_train_steps: -1\n",
            "02:20:23 |     max_train_time: 600.0\n",
            "02:20:23 |     metrics: default\n",
            "02:20:23 |     model: transformer/generator\n",
            "02:20:23 |     model_file: from_pretrained/model\n",
            "02:20:23 |     model_parallel: False\n",
            "02:20:23 |     momentum: 0\n",
            "02:20:23 |     multitask_weights: [1]\n",
            "02:20:23 |     mutators: None\n",
            "02:20:23 |     n_decoder_layers: -1\n",
            "02:20:23 |     n_encoder_layers: -1\n",
            "02:20:23 |     n_heads: 16\n",
            "02:20:23 |     n_layers: 8\n",
            "02:20:23 |     n_positions: 512\n",
            "02:20:23 |     n_segments: 0\n",
            "02:20:23 |     nesterov: True\n",
            "02:20:23 |     no_cuda: False\n",
            "02:20:23 |     num_epochs: -1\n",
            "02:20:23 |     num_workers: 0\n",
            "02:20:23 |     nus: (0.7,)\n",
            "02:20:23 |     optimizer: mem_eff_adam\n",
            "02:20:23 |     output_scaling: 1.0\n",
            "02:20:23 |     override: \"{'task': 'empathetic_dialogues', 'model': 'transformer/generator', 'model_file': 'from_pretrained/model', 'init_model': 'zoo:tutorial_transformer_generator/model', 'n_heads': 16, 'n_layers': 8, 'n_positions': 512, 'text_truncate': 512, 'label_truncate': 128, 'ffn_size': 2048, 'embedding_size': 512, 'activation': 'gelu', 'variant': 'xlm', 'dict_lower': True, 'dict_tokenizer': 'bpe', 'dict_file': '/usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model.dict', 'learn_positional_embeddings': True, 'learningrate': 1e-05, 'optimizer': 'adam', 'warmup_updates': 100, 'validation_metric': 'ppl', 'max_train_time': 600.0, 'validation_every_n_epochs': 0.25, 'batchsize': 12, 'fp16': True, 'fp16_impl': 'mem_efficient', 'skip_generation': True, 'dynamic_batching': 'full'}\"\n",
            "02:20:23 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "02:20:23 |     person_tokens: False\n",
            "02:20:23 |     rank_candidates: False\n",
            "02:20:23 |     relu_dropout: 0.0\n",
            "02:20:23 |     remove_political_convos: False\n",
            "02:20:23 |     save_after_valid: False\n",
            "02:20:23 |     save_every_n_secs: -1\n",
            "02:20:23 |     share_word_embeddings: True\n",
            "02:20:23 |     short_final_eval: False\n",
            "02:20:23 |     skip_generation: True\n",
            "02:20:23 |     special_tok_lst: None\n",
            "02:20:23 |     split_lines: False\n",
            "02:20:23 |     starttime: Jul21_02-20\n",
            "02:20:23 |     task: empathetic_dialogues\n",
            "02:20:23 |     temperature: 1.0\n",
            "02:20:23 |     tensorboard_log: False\n",
            "02:20:23 |     tensorboard_logdir: None\n",
            "02:20:23 |     text_truncate: 512\n",
            "02:20:23 |     topk: 10\n",
            "02:20:23 |     topp: 0.9\n",
            "02:20:23 |     train_experiencer_only: False\n",
            "02:20:23 |     truncate: -1\n",
            "02:20:23 |     update_freq: 1\n",
            "02:20:23 |     use_reply: label\n",
            "02:20:23 |     validation_cutoff: 1.0\n",
            "02:20:23 |     validation_every_n_epochs: 0.25\n",
            "02:20:23 |     validation_every_n_secs: -1\n",
            "02:20:23 |     validation_every_n_steps: -1\n",
            "02:20:23 |     validation_max_exs: -1\n",
            "02:20:23 |     validation_metric: ppl\n",
            "02:20:23 |     validation_metric_mode: None\n",
            "02:20:23 |     validation_patience: 10\n",
            "02:20:23 |     validation_share_agent: False\n",
            "02:20:23 |     variant: xlm\n",
            "02:20:23 |     verbose: False\n",
            "02:20:23 |     wandb_entity: None\n",
            "02:20:23 |     wandb_log: False\n",
            "02:20:23 |     wandb_name: None\n",
            "02:20:23 |     wandb_project: None\n",
            "02:20:23 |     warmup_rate: 0.0001\n",
            "02:20:23 |     warmup_updates: 100\n",
            "02:20:23 |     weight_decay: None\n",
            "02:20:23 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? False, datatype: train\n",
            "02:20:28 | training...\n",
            "02:20:29 | Overflow: setting loss scale to 65536.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/parlai/utils/fp16.py:85: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  return torch.nn.utils.clip_grad_norm_(params, max_norm)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "02:20:43 | time:14s total_exs:5308 total_steps:50 epochs:0.08\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n",
            "    26.3 .9800  2792  9972       0          0 379.1 5308             65536  4.491    .5298 16.03 2.849 5.001e-06  1702  6078   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 17.27      .3948  .0001884                   50 4494 16050 3.572\n",
            "\n",
            "02:20:52 | Overflow: setting loss scale to 32768.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/parlai/utils/fp16.py:85: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  return torch.nn.utils.clip_grad_norm_(params, max_norm)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "02:20:56 | time:28s total_exs:9808 total_steps:100 epochs:0.15\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   32.84 .9800  2956 10903       0          0   332 4500             54395  4.088    .5350  16.8 2.772 9.9e-06  1512  5578   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 15.99      .4048  .0002222                  100 4468 16481 3.689\n",
            "\n",
            "02:21:11 | time:42s total_exs:14596 total_steps:150 epochs:0.23\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   31.98     1  3062 10557       0          0 330.1 4788             32768  3.963    .5340  17.3 2.729 9.9e-06  1656  5710   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 15.32      .4089  .0006266                  150 4718 16267 3.448\n",
            "\n",
            "02:21:16 | time:48s total_exs:16252 total_steps:168 epochs:0.25\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   30.06     1  2765  9853       0          0 327.8 1656             32768  4.175    .5340 16.45 2.701 9.9e-06  1514  5392   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 14.89      .4121         0                  168 4279 15245 3.566\n",
            "\n",
            "02:21:16 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "02:21:18 | running eval: valid\n",
            "02:21:19 | \u001b[33m--skip-generation true produces limited metrics\u001b[0m\n",
            "02:21:25 | eval completed in 6.96s\n",
            "02:21:25 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   40.45  3517 34836       0          0 861.2 5738    .1009 16.01 2.534 9.9e-06  1392 13788       0          0 12.61   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4314  .0008714                  168 4909 48624\n",
            "\u001b[0m\n",
            "02:21:25 | \u001b[1;32mnew best ppl: 12.61\u001b[0m\n",
            "02:21:25 | saving best valid model: from_pretrained/model\n",
            "02:21:25 | Saving dictionary to from_pretrained/model.dict\n",
            "02:21:43 | time:75s total_exs:21192 total_steps:218 epochs:0.33\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   32.55     1  3216 11806       0          0 362.7 4940             32768  4.264    .5211 16.21 2.666 9.9e-06  1601  5879   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 14.38      .4160  .0006073                  218 4817 17685 3.672\n",
            "\n",
            "02:21:57 | time:89s total_exs:26060 total_steps:268 epochs:0.40\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   29.99     1  2920 10259       0          0   342 4868             32768  4.166    .5211 17.22 2.717 9.9e-06  1676  5889   \n",
            "     ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "   .0002054   .0002054 15.14      .4092  .0004108                  268 4596 16148 3.514\n",
            "\n",
            "02:22:12 | time:103s total_exs:30828 total_steps:318 epochs:0.48\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   30.97     1  2954 10329       0          0 333.5 4768             32768  4.153    .5340  16.5 2.692 9.9e-06  1573  5500   \n",
            "     ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "   .0002097    .003775 14.76      .4130  .0002097                  318 4526 15829 3.498\n",
            "\n",
            "02:22:17 | time:108s total_exs:32484 total_steps:335 epochs:0.50\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "    26.8     1  2611  9185       0          0 342.6 1656             32768  3.982    .4550 17.25 2.712 9.9e-06  1680  5910   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "         0          0 15.06      .4153         0                  335 4291 15096 3.52\n",
            "\n",
            "02:22:17 | running eval: valid\n",
            "02:22:23 | eval completed in 6.51s\n",
            "02:22:23 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   40.45  3571 36970       0          0 913.9 5738    .1009 16.01 2.504 9.9e-06  1413 14633       0          0 12.23   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4362   .001046                  335 4984 51603\n",
            "\u001b[0m\n",
            "02:22:23 | \u001b[1;32mnew best ppl: 12.23 (previous best was 12.61)\u001b[0m\n",
            "02:22:23 | saving best valid model: from_pretrained/model\n",
            "02:22:41 | time:133s total_exs:37112 total_steps:385 epochs:0.57\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   33.09     1  3063 10859       0          0 328.1 4628             32768  4.316    .5298 16.67 2.674 9.9e-06  1543  5469   \n",
            "     ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "   .0002161   .0002161 14.5      .4151  .0008643                  385 4606 16327 3.545\n",
            "\n",
            "02:22:56 | time:147s total_exs:41760 total_steps:435 epochs:0.65\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   30.32     1  2818  9930       0          0 327.5 4648             32768  4.219    .5163 16.95 2.664 9.9e-06  1576  5552   \n",
            "     ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "   .0002151    .001721 14.35      .4151  .0002151                  435 4394 15482 3.524\n",
            "\n",
            "02:23:10 | time:162s total_exs:46152 total_steps:485 epochs:0.71\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   35.82     1  3147 11051       0          0 308.5 4392             32768  4.329    .5162  17.1 2.659 9.9e-06  1502  5277   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 14.28      .4182  .0004554                  485 4649 16328 3.513\n",
            "\n",
            "02:23:17 | time:169s total_exs:48672 total_steps:509 epochs:0.75\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "    29.8     1  3129 10742       0          0 360.4 2520             32768  3.973    .5163    16 2.619 9.9e-06  1680  5765   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 13.72      .4222  .0003968                  509 4809 16507 3.434\n",
            "\n",
            "02:23:17 | running eval: valid\n",
            "02:23:24 | eval completed in 6.88s\n",
            "02:23:24 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   40.45  3627 35081       0          0 867.3 5738    .1008 16.01 2.488 9.9e-06  1435 13885       0          0 12.04   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4387   .001046                  509 5062 48967\n",
            "\u001b[0m\n",
            "02:23:24 | \u001b[1;32mnew best ppl: 12.04 (previous best was 12.23)\u001b[0m\n",
            "02:23:24 | saving best valid model: from_pretrained/model\n",
            "02:23:43 | time:194s total_exs:53332 total_steps:559 epochs:0.83\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   29.93     1  2790  9629       0          0 321.7 4660             32768   4.11    .5429 17.05 2.693 9.9e-06  1589  5484   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 14.78      .4105  .0002146                  559 4379 15112 3.452\n",
            "\n",
            "02:23:57 | time:209s total_exs:58068 total_steps:609 epochs:0.90\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   30.79     1  2916 10113       0          0 328.5 4736             32768  4.146    .5095 16.25 2.664 9.9e-06  1540  5339   \n",
            "     ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "   .0002111   .0002111 14.35      .4192         0                  609 4456 15452 3.468\n",
            "\n",
            "02:24:12 | time:223s total_exs:63024 total_steps:659 epochs:0.98\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   30.24     1  2997 10480       0          0 346.6 4956             32768   4.02    .5298 16.62 2.648 9.9e-06  1647  5758   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 14.12      .4189  .0004036                  659 4644 16239 3.497\n",
            "\n",
            "02:24:17 | time:229s total_exs:64860 total_steps:678 epochs:1.00\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   32.41     1  3132 10826       0          0   334 1836             32768  4.083    .5005 17.04 2.652 9.9e-06  1647  5692   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 14.18      .4204         0                  678 4779 16518 3.458\n",
            "\n",
            "02:24:17 | running eval: valid\n",
            "02:24:24 | eval completed in 6.90s\n",
            "02:24:24 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   40.45  3571 35165       0          0 869.3 5738    .1008 16.01 2.478 9.9e-06  1413 13918       0          0 11.92   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4403    .00122                  678 4984 49084\n",
            "\u001b[0m\n",
            "02:24:24 | \u001b[1;32mnew best ppl: 11.92 (previous best was 12.04)\u001b[0m\n",
            "02:24:24 | saving best valid model: from_pretrained/model\n",
            "02:24:42 | time:254s total_exs:69312 total_steps:728 epochs:1.07\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   33.68     1  2999 10725       0          0 318.4 4452             32768  4.296    .5340 16.99 2.629 9.9e-06  1513  5410   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 13.86      .4222  .0002246                  728 4512 16135 3.577\n",
            "\n",
            "02:24:57 | time:269s total_exs:73892 total_steps:778 epochs:1.14\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "    33.1     1  3032 10189       0          0 307.9 4580             32768   4.09    .5340 17.34 2.657 9.9e-06  1588  5337   \n",
            "     ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "   .0002183     .00393 14.25      .4181         0                  778 4619 15526 3.362\n",
            "\n",
            "02:25:12 | time:283s total_exs:79024 total_steps:828 epochs:1.22\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "    28.5     1  2926 10154       0          0 356.2 5132             32768  4.074    .5061 16.14 2.614 9.9e-06  1657  5751   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 13.65      .4215  .0003897                  828 4583 15905 3.472\n",
            "\n",
            "02:25:19 | time:291s total_exs:81052 total_steps:853 epochs:1.25\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "    35.7     1  2896  9817       0          0   275 2028             32768  4.335    .5266 17.21 2.619 9.9e-06  1396  4733   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 13.73      .4247   .001479                  853 4292 14550 3.391\n",
            "\n",
            "02:25:19 | running eval: valid\n",
            "02:25:26 | eval completed in 6.82s\n",
            "02:25:26 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   40.45  3464 35395       0          0   875 5738    .1009 16.01  2.47 9.9e-06  1371 14009       0          0 11.82   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4416  .0008714                  853 4835 49405\n",
            "\u001b[0m\n",
            "02:25:26 | \u001b[1;32mnew best ppl: 11.82 (previous best was 11.92)\u001b[0m\n",
            "02:25:26 | saving best valid model: from_pretrained/model\n",
            "02:25:45 | time:317s total_exs:85820 total_steps:903 epochs:1.33\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   30.58     1  2916 10003       0          0 327.1 4768             32768  4.394    .5088 16.57 2.611 9.9e-06  1580  5421   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 13.62      .4226  .0006292                  903 4497 15424 3.431\n",
            "\n",
            "02:25:59 | time:331s total_exs:90600 total_steps:953 epochs:1.40\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   32.11     1  3069 10597       0          0 330.1 4780             32768  4.111    .5266 16.54 2.597 9.9e-06  1581  5458   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 13.42      .4250  .0004184                  953 4650 16056 3.454\n",
            "\n",
            "02:26:14 | time:346s total_exs:95292 total_steps:1003 epochs:1.47\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   31.93     1  2996 10062       0          0 315.2 4692             32768  4.039    .5084 17.42  2.62 9.9e-06  1635  5491   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 13.74      .4216         0                 1003 4631 15554 3.359\n",
            "\n",
            "02:26:20 | time:352s total_exs:97224 total_steps:1023 epochs:1.50\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "    32.5     1  3140 10547       0          0 324.4 1932             32768  4.166    .5211 16.12 2.569 9.9e-06  1557  5231   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "         0          0 13.05      .4278  .0005176                 1023 4697 15778 3.36\n",
            "\n",
            "02:26:20 | running eval: valid\n",
            "02:26:27 | eval completed in 6.85s\n",
            "02:26:27 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   40.45  3517 35075       0          0 867.1 5738    .1008 16.01 2.464 9.9e-06  1392 13883       0          0 11.75   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4418  .0006971                 1023 4909 48958\n",
            "\u001b[0m\n",
            "02:26:27 | \u001b[1;32mnew best ppl: 11.75 (previous best was 11.82)\u001b[0m\n",
            "02:26:27 | saving best valid model: from_pretrained/model\n",
            "02:26:46 | time:378s total_exs:102212 total_steps:1073 epochs:1.58\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   30.99     1  3092 10390       0          0 335.3 4988             32768  4.104    .5061 16.77 2.619 9.9e-06  1673  5624   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 13.72      .4217  .0006014                 1073 4765 16014 3.361\n",
            "\n",
            "02:27:00 | time:392s total_exs:107072 total_steps:1123 epochs:1.66\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   30.51     1  2965 10558       0          0 346.1 4860             32768  4.167    .5211  16.3 2.581 9.9e-06  1585  5642   \n",
            "     ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "   .0002058    .001646 13.2      .4258  .0004115                 1123 4550 16200 3.561\n",
            "\n",
            "02:27:08 | Overflow: setting loss scale to 16384.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/parlai/utils/fp16.py:85: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  return torch.nn.utils.clip_grad_norm_(params, max_norm)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "02:27:15 | time:407s total_exs:111932 total_steps:1173 epochs:1.73\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   29.74 .9800  2891  9779       0          0 328.7 4860             24576  3.951    .5298  16.6   2.6 9.9e-06  1614  5458   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 13.46      .4247   .000823                 1173 4505 15236 3.383\n",
            "\n",
            "02:27:20 | time:411s total_exs:113432 total_steps:1189 epochs:1.75\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   34.06     1  3194 11468       0          0 336.7 1500             16384  4.274    .5061 15.65 2.575 9.9e-06  1467  5268   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 13.13      .4291         0                 1189 4660 16736 3.594\n",
            "\n",
            "02:27:20 | running eval: valid\n",
            "02:27:27 | eval completed in 6.95s\n",
            "02:27:27 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   40.45  3517 34759       0          0 859.3 5738    .1008 16.01  2.46 9.9e-06  1392 13758       0          0 11.7   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4427   .001046                 1189 4909 48517\n",
            "\u001b[0m\n",
            "02:27:27 | \u001b[1;32mnew best ppl: 11.7 (previous best was 11.75)\u001b[0m\n",
            "02:27:27 | saving best valid model: from_pretrained/model\n",
            "02:27:45 | time:437s total_exs:118288 total_steps:1239 epochs:1.83\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   33.17     1  3222 11384       0          0 343.2 4856             16384  4.147    .4950 16.36 2.578 9.9e-06  1588  5613   \n",
            "     ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "   .0002059    .001647 13.17      .4254  .0006178                 1239 4810 16997 3.534\n",
            "\n",
            "02:28:00 | time:451s total_exs:122864 total_steps:1289 epochs:1.90\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   32.08     1  2936 10189       0          0 317.6 4576             16384  4.031    .5429 17.02   2.6 9.9e-06  1557  5404   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "         0          0 13.46      .4232  .0002185                 1289 4494 15593 3.47\n",
            "\n",
            "02:28:14 | time:465s total_exs:127844 total_steps:1339 epochs:1.98\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   29.83     1  2971 10467       0          0 350.8 4980             16384  4.085    .5350 16.37 2.586 9.9e-06  1631  5743   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 13.27      .4248         0                 1339 4602 16210 3.523\n",
            "\n",
            "02:28:19 | time:470s total_exs:129644 total_steps:1357 epochs:2.01\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   27.01     1  2701  9821       0          0 363.6 1800             16384  3.956    .5084 16.69 2.616 9.9e-06  1669  6067   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 13.69      .4214  .0005556                 1357 4370 15888 3.638\n",
            "\n",
            "02:28:19 | running eval: valid\n",
            "02:28:26 | eval completed in 7.06s\n",
            "02:28:26 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   40.45  3517 35609       0          0 880.3 5738    .1009 16.01 2.454 9.9e-06  1392 14094       0          0 11.63   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4436   .001046                 1357 4909 49703\n",
            "\u001b[0m\n",
            "02:28:26 | \u001b[1;32mnew best ppl: 11.63 (previous best was 11.7)\u001b[0m\n",
            "02:28:26 | saving best valid model: from_pretrained/model\n",
            "02:28:45 | time:497s total_exs:134084 total_steps:1407 epochs:2.07\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   34.37     1  3052 10317       0          0 300.2 4440             16384  4.268    .5298 16.99 2.575 9.9e-06  1508  5099   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 13.14      .4260   .001351                 1407 4560 15416 3.381\n",
            "\n",
            "02:29:00 | time:511s total_exs:138752 total_steps:1457 epochs:2.15\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   31.22     1  2915  9956       0          0 318.9 4668             16384  4.056    .5211 16.89 2.564 9.9e-06  1576  5384   \n",
            "     ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "   .0002142    .003856 12.98      .4277  .0002142                 1457 4491 15340 3.416\n",
            "\n",
            "02:29:14 | time:526s total_exs:143524 total_steps:1507 epochs:2.22\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   31.47     1  3004 10280       0          0 326.6 4772             16384  4.308    .5331 16.57 2.576 9.9e-06  1581  5412   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 13.14      .4290  .0004191                 1507 4585 15692 3.423\n",
            "\n",
            "02:29:21 | time:533s total_exs:145916 total_steps:1531 epochs:2.26\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   29.78     1  2968 10258       0          0 344.5 2392             16384  3.963    .5219 16.01 2.568 9.9e-06  1596  5515   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 13.04      .4269  .0008361                 1531 4564 15773 3.458\n",
            "\n",
            "02:29:21 | running eval: valid\n",
            "02:29:28 | eval completed in 7.24s\n",
            "02:29:28 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   40.45  3464 33454       0          0   827 5738    .1009 16.01 2.451 9.9e-06  1371 13241       0          0 11.6   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4440   .001046                 1531 4835 46695\n",
            "\u001b[0m\n",
            "02:29:28 | \u001b[1;32mnew best ppl: 11.6 (previous best was 11.63)\u001b[0m\n",
            "02:29:28 | saving best valid model: from_pretrained/model\n",
            "02:29:47 | time:559s total_exs:150600 total_steps:1581 epochs:2.33\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   32.18     1  3015 10343       0          0 321.4 4684             16384  4.141    .5298 16.87 2.595 9.9e-06  1581  5422   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 13.4      .4243   .001067                 1581 4596 15766 3.431\n",
            "\n",
            "02:30:01 | time:573s total_exs:155116 total_steps:1631 epochs:2.40\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   32.57     1  2942 10522       0          0   323 4516             16384  4.275    .5331 16.81 2.574 9.9e-06  1519  5431   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 13.12      .4269   .001329                 1631 4461 15953 3.577\n",
            "\n",
            "02:30:16 | time:588s total_exs:159764 total_steps:1681 epochs:2.47\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   32.88     1  3057 10461       0          0 318.1 4648             16384  4.482    .5061 16.95 2.582 9.9e-06  1576  5394   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 13.23      .4292  .0002151                 1681 4633 15855 3.423\n",
            "\n",
            "02:30:23 | time:594s total_exs:162172 total_steps:1704 epochs:2.51\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   26.41     1  2765  9591       0          0 363.1 2408             16384  4.202    .5211 15.99 2.534 9.9e-06  1674  5805   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 12.61      .4290   .001246                 1704 4439 15396 3.471\n",
            "\n",
            "02:30:23 | running eval: valid\n",
            "02:30:30 | eval completed in 7.02s\n",
            "02:30:30 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   40.45  3517 34325       0          0 848.6 5738    .1008 16.01 2.447 9.9e-06  1392 13586       0          0 11.55   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4440   .001046                 1704 4909 47911\n",
            "\u001b[0m\n",
            "02:30:30 | \u001b[1;32mnew best ppl: 11.55 (previous best was 11.6)\u001b[0m\n",
            "02:30:30 | saving best valid model: from_pretrained/model\n",
            "02:30:34 | max_train_time elapsed:606.0612156391144s\n",
            "02:30:34 | \u001b[33mOverriding opt[\"init_model\"] to zoo:tutorial_transformer_generator/model (previously: /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model)\u001b[0m\n",
            "02:30:34 | \u001b[33mOverriding opt[\"optimizer\"] to adam (previously: mem_eff_adam)\u001b[0m\n",
            "02:30:34 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: full,is_debug: False,eval_dynamic_batching: None,num_workers: 0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_steps: -1,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,mutators: None,train_experiencer_only: False,remove_political_convos: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,beam_block_full_context: True,beam_length_penalty: 0.65,topk: 10,topp: 0.9,beam_delay: 30,beam_block_list_filename: None,temperature: 1.0,compute_tokenized_bleu: False,fp16_impl: mem_efficient,force_fp16_tokens: True,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,parlai_home: /usr/local/lib/python3.7/dist-packages,dict_loaded: True,download_path: None,verbose: False,datapath: /usr/local/lib/python3.7/dist-packages/data,load_from_checkpoint: True,interactive_mode: False\u001b[0m\n",
            "02:30:34 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--show-advanced-args False --task internal:new_reddit:presorted --datatype train:stream --numthreads 1 --batchsize 48 --num-epochs 5.0 --max-train-time -1 --validation-every-n-secs 1800.0 --save-after-valid True --validation-every-n-epochs -1 --validation-max-exs 9920 --short-final-eval True --validation-patience 0 --validation-metric-mode min --dict-build-first True --numworkers 4 --pytorch-preprocess False --pytorch-teacher-batch-sort False --batch-sort-cache-type pop --batch-length-range 5 --shuffle False --batch-sort-field text --pytorch-context-length -1 --pytorch-include-labels True --log-every-n-secs 30.0 --distributed-world-size 64 --port 61337 --dropout 0.1 --beam-size 8 --beam-min-n-best 3 --beam-min-length 10 --skip-generation False --inference beam --optimizer fused_adam --learningrate 0.0005 --gradient-clip 10.0 --adam-eps 1e-06 --betas 0.9,0.98 --weight-decay 0.01 --lr-scheduler invsqrt --warmup-updates 20000 --gpu 0 --beam-block-ngram 3 --beam-context-block-ngram 3\u001b[0m\n",
            "02:30:34 | Using CUDA\n",
            "02:30:34 | loading dictionary from from_pretrained/model.dict\n",
            "02:30:35 | num words = 54944\n",
            "02:30:36 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "02:30:36 | Loading existing model params from from_pretrained/model\n",
            "02:30:39 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "02:30:41 | running eval: valid\n",
            "02:30:48 | eval completed in 7.01s\n",
            "02:30:48 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   40.45  3464 34787       0          0 859.9 5738   .08969 16.01 2.447 9.9e-06  1371 13768       0          0 11.55   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4439   .001046                 1704 4835 48556\n",
            "\u001b[0m\n",
            "02:30:49 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: test\n",
            "02:30:52 | running eval: test\n",
            "02:30:59 | eval completed in 6.72s\n",
            "02:30:59 | \u001b[1mtest:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   43.69  3590 35879       0          0 821.3 5259   .08967 16.23  2.47 9.9e-06  1334 13329       0          0 11.82   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4409  .0007606                 1704 4923 49208\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'clen': AverageMetric(40.45),\n",
              "  'ctpb': GlobalAverageMetric(3464),\n",
              "  'ctps': GlobalTimerMetric(3.479e+04),\n",
              "  'ctrunc': AverageMetric(0),\n",
              "  'ctrunclen': AverageMetric(0),\n",
              "  'exps': GlobalTimerMetric(859.9),\n",
              "  'exs': SumMetric(5738),\n",
              "  'gpu_mem': GlobalAverageMetric(0.08969),\n",
              "  'llen': AverageMetric(16.01),\n",
              "  'loss': AverageMetric(2.447),\n",
              "  'lr': GlobalAverageMetric(9.9e-06),\n",
              "  'ltpb': GlobalAverageMetric(1371),\n",
              "  'ltps': GlobalTimerMetric(1.377e+04),\n",
              "  'ltrunc': AverageMetric(0),\n",
              "  'ltrunclen': AverageMetric(0),\n",
              "  'ppl': PPLMetric(11.55),\n",
              "  'token_acc': AverageMetric(0.4439),\n",
              "  'token_em': AverageMetric(0.001046),\n",
              "  'total_train_updates': GlobalFixedMetric(1704),\n",
              "  'tpb': GlobalAverageMetric(4835),\n",
              "  'tps': GlobalTimerMetric(4.856e+04)},\n",
              " {'clen': AverageMetric(43.69),\n",
              "  'ctpb': GlobalAverageMetric(3590),\n",
              "  'ctps': GlobalTimerMetric(3.588e+04),\n",
              "  'ctrunc': AverageMetric(0),\n",
              "  'ctrunclen': AverageMetric(0),\n",
              "  'exps': GlobalTimerMetric(821.3),\n",
              "  'exs': SumMetric(5259),\n",
              "  'gpu_mem': GlobalAverageMetric(0.08967),\n",
              "  'llen': AverageMetric(16.23),\n",
              "  'loss': AverageMetric(2.47),\n",
              "  'lr': GlobalAverageMetric(9.9e-06),\n",
              "  'ltpb': GlobalAverageMetric(1334),\n",
              "  'ltps': GlobalTimerMetric(1.333e+04),\n",
              "  'ltrunc': AverageMetric(0),\n",
              "  'ltrunclen': AverageMetric(0),\n",
              "  'ppl': PPLMetric(11.82),\n",
              "  'token_acc': AverageMetric(0.4409),\n",
              "  'token_em': AverageMetric(0.0007606),\n",
              "  'total_train_updates': GlobalFixedMetric(1704),\n",
              "  'tpb': GlobalAverageMetric(4923),\n",
              "  'tps': GlobalTimerMetric(4.921e+04)})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iBZXTLRvIjb"
      },
      "source": [
        "## Wow that's a lot of options? Where do I find more info?\n",
        "\n",
        "As you might have noticed, there are a LOT of options to ParlAI. You're best reading the [ParlAI docs](https://parl.ai/docs) to find a list of hyperparameters. We provide lists of the command-line args for both models\n",
        "\n",
        "You can get some guidance in this notebook by using:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Pl8VVl5plfm"
      },
      "source": [
        "# note that if you want to see model-specific arguments, you must specify a model name\n",
        "print(TrainModel.help(model='seq2seq'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKGUWyKTwVtX"
      },
      "source": [
        "You'll notice the options are give as commandline arguments. We control our options via `argparse`. The option names are relatively predictable: `--init-model` becomes `init_model`; `--num-epochs` becomes `num_epochs` and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgLwGAq1wZJb"
      },
      "source": [
        "# Looking at model predictions\n",
        "\n",
        "We have shown how we can chat with a model ourselves, interactively. We might want to inspect how the model reacts with a fixed set of inputs. Let's use that model we just trained!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCZgs6OlvJ-q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff9b6921-a54c-4b2e-8c3f-d4c468ea08af"
      },
      "source": [
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    task='empathetic_dialogues',\n",
        "    model_file='from_pretrained/model',\n",
        "    num_examples=2,\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02:32:55 | Using CUDA\n",
            "02:32:55 | loading dictionary from from_pretrained/model.dict\n",
            "02:32:55 | num words = 54944\n",
            "02:32:56 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "02:32:56 | Loading existing model params from from_pretrained/model\n",
            "02:33:07 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "02:33:07 | Opt:\n",
            "02:33:07 |     activation: gelu\n",
            "02:33:07 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "02:33:07 |     adam_eps: 1e-08\n",
            "02:33:07 |     add_p1_after_newln: False\n",
            "02:33:07 |     aggregate_micro: False\n",
            "02:33:07 |     allow_missing_init_opts: False\n",
            "02:33:08 |     attention_dropout: 0.0\n",
            "02:33:08 |     batchsize: 12\n",
            "02:33:08 |     beam_block_full_context: True\n",
            "02:33:08 |     beam_block_list_filename: None\n",
            "02:33:08 |     beam_block_ngram: -1\n",
            "02:33:08 |     beam_context_block_ngram: -1\n",
            "02:33:08 |     beam_delay: 30\n",
            "02:33:08 |     beam_length_penalty: 0.65\n",
            "02:33:08 |     beam_min_length: 1\n",
            "02:33:08 |     beam_size: 1\n",
            "02:33:08 |     betas: '[0.9, 0.999]'\n",
            "02:33:08 |     bpe_add_prefix_space: None\n",
            "02:33:08 |     bpe_debug: False\n",
            "02:33:08 |     bpe_dropout: None\n",
            "02:33:08 |     bpe_merge: None\n",
            "02:33:08 |     bpe_vocab: None\n",
            "02:33:08 |     compute_tokenized_bleu: False\n",
            "02:33:08 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "02:33:08 |     datatype: train\n",
            "02:33:08 |     delimiter: '\\n'\n",
            "02:33:08 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "02:33:08 |     dict_endtoken: __end__\n",
            "02:33:08 |     dict_file: from_pretrained/model.dict\n",
            "02:33:08 |     dict_include_test: False\n",
            "02:33:08 |     dict_include_valid: False\n",
            "02:33:08 |     dict_initpath: None\n",
            "02:33:08 |     dict_language: english\n",
            "02:33:08 |     dict_loaded: True\n",
            "02:33:08 |     dict_lower: True\n",
            "02:33:08 |     dict_max_ngram_size: -1\n",
            "02:33:08 |     dict_maxexs: -1\n",
            "02:33:08 |     dict_maxtokens: -1\n",
            "02:33:08 |     dict_minfreq: 0\n",
            "02:33:08 |     dict_nulltoken: __null__\n",
            "02:33:08 |     dict_starttoken: __start__\n",
            "02:33:08 |     dict_textfields: text,labels\n",
            "02:33:08 |     dict_tokenizer: bpe\n",
            "02:33:08 |     dict_unktoken: __unk__\n",
            "02:33:08 |     display_add_fields: \n",
            "02:33:08 |     display_examples: False\n",
            "02:33:08 |     download_path: None\n",
            "02:33:08 |     dropout: 0.0\n",
            "02:33:08 |     dynamic_batching: full\n",
            "02:33:08 |     embedding_projection: random\n",
            "02:33:08 |     embedding_size: 512\n",
            "02:33:08 |     embedding_type: random\n",
            "02:33:08 |     embeddings_scale: True\n",
            "02:33:08 |     eval_batchsize: None\n",
            "02:33:08 |     eval_dynamic_batching: None\n",
            "02:33:08 |     evaltask: None\n",
            "02:33:08 |     ffn_size: 2048\n",
            "02:33:08 |     force_fp16_tokens: True\n",
            "02:33:08 |     fp16: True\n",
            "02:33:08 |     fp16_impl: mem_efficient\n",
            "02:33:08 |     gpu: -1\n",
            "02:33:08 |     gradient_clip: 0.1\n",
            "02:33:08 |     hide_labels: False\n",
            "02:33:08 |     history_add_global_end_token: None\n",
            "02:33:08 |     history_reversed: False\n",
            "02:33:08 |     history_size: -1\n",
            "02:33:08 |     image_cropsize: 224\n",
            "02:33:08 |     image_mode: raw\n",
            "02:33:08 |     image_size: 256\n",
            "02:33:08 |     inference: greedy\n",
            "02:33:08 |     init_model: /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "02:33:08 |     init_opt: None\n",
            "02:33:08 |     interactive_mode: False\n",
            "02:33:08 |     invsqrt_lr_decay_gamma: -1\n",
            "02:33:08 |     is_debug: False\n",
            "02:33:08 |     label_truncate: 128\n",
            "02:33:08 |     learn_positional_embeddings: True\n",
            "02:33:08 |     learningrate: 1e-05\n",
            "02:33:08 |     log_every_n_secs: -1\n",
            "02:33:08 |     log_every_n_steps: 50\n",
            "02:33:08 |     loglevel: info\n",
            "02:33:08 |     lr_scheduler: reduceonplateau\n",
            "02:33:08 |     lr_scheduler_decay: 0.5\n",
            "02:33:08 |     lr_scheduler_patience: 3\n",
            "02:33:08 |     max_train_steps: -1\n",
            "02:33:08 |     max_train_time: 600.0\n",
            "02:33:08 |     metrics: default\n",
            "02:33:08 |     model: transformer/generator\n",
            "02:33:08 |     model_file: from_pretrained/model\n",
            "02:33:08 |     model_parallel: False\n",
            "02:33:08 |     momentum: 0\n",
            "02:33:08 |     multitask_weights: [1]\n",
            "02:33:08 |     mutators: None\n",
            "02:33:08 |     n_decoder_layers: -1\n",
            "02:33:08 |     n_encoder_layers: -1\n",
            "02:33:08 |     n_heads: 16\n",
            "02:33:08 |     n_layers: 8\n",
            "02:33:08 |     n_positions: 512\n",
            "02:33:08 |     n_segments: 0\n",
            "02:33:08 |     nesterov: True\n",
            "02:33:08 |     no_cuda: False\n",
            "02:33:08 |     num_epochs: -1\n",
            "02:33:08 |     num_examples: 2\n",
            "02:33:08 |     num_workers: 0\n",
            "02:33:08 |     nus: [0.7]\n",
            "02:33:08 |     optimizer: mem_eff_adam\n",
            "02:33:08 |     output_scaling: 1.0\n",
            "02:33:08 |     override: \"{'task': 'empathetic_dialogues', 'model_file': 'from_pretrained/model', 'num_examples': '2'}\"\n",
            "02:33:08 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "02:33:08 |     person_tokens: False\n",
            "02:33:08 |     rank_candidates: False\n",
            "02:33:08 |     relu_dropout: 0.0\n",
            "02:33:08 |     remove_political_convos: False\n",
            "02:33:08 |     save_after_valid: False\n",
            "02:33:08 |     save_every_n_secs: -1\n",
            "02:33:08 |     share_word_embeddings: True\n",
            "02:33:08 |     short_final_eval: False\n",
            "02:33:08 |     skip_generation: True\n",
            "02:33:08 |     special_tok_lst: None\n",
            "02:33:08 |     split_lines: False\n",
            "02:33:08 |     starttime: Jul21_02-20\n",
            "02:33:08 |     task: empathetic_dialogues\n",
            "02:33:08 |     temperature: 1.0\n",
            "02:33:08 |     tensorboard_log: False\n",
            "02:33:08 |     tensorboard_logdir: None\n",
            "02:33:08 |     text_truncate: 512\n",
            "02:33:08 |     topk: 10\n",
            "02:33:08 |     topp: 0.9\n",
            "02:33:08 |     train_experiencer_only: False\n",
            "02:33:08 |     truncate: -1\n",
            "02:33:08 |     update_freq: 1\n",
            "02:33:08 |     use_reply: label\n",
            "02:33:08 |     validation_cutoff: 1.0\n",
            "02:33:08 |     validation_every_n_epochs: 0.25\n",
            "02:33:08 |     validation_every_n_secs: -1\n",
            "02:33:08 |     validation_every_n_steps: -1\n",
            "02:33:08 |     validation_max_exs: -1\n",
            "02:33:08 |     validation_metric: ppl\n",
            "02:33:08 |     validation_metric_mode: None\n",
            "02:33:08 |     validation_patience: 10\n",
            "02:33:08 |     validation_share_agent: False\n",
            "02:33:08 |     variant: xlm\n",
            "02:33:08 |     verbose: False\n",
            "02:33:08 |     wandb_entity: None\n",
            "02:33:08 |     wandb_log: False\n",
            "02:33:08 |     wandb_name: None\n",
            "02:33:08 |     wandb_project: None\n",
            "02:33:08 |     warmup_rate: 0.0001\n",
            "02:33:08 |     warmup_updates: 100\n",
            "02:33:08 |     weight_decay: None\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues- - -\u001b[0;0m\n",
            "\u001b[0mToday,as i was leaving for work in the morning,i had a tire burst in the middle of a busy road. That scared the hell out of me!\u001b[0;0m\n",
            "\u001b[1;94m    labels: Are you fine now?\u001b[0;0m\n",
            "\u001b[0;95m     model: No response\u001b[0;0m\n",
            "\u001b[0mYeah,i'm doing alright now, but with minor injuries.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Cool :) Is your car damaged a lot?\u001b[0;0m\n",
            "\u001b[0;95m     model: No response\u001b[0;0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H3QKTjdwokh"
      },
      "source": [
        "Whoa wait a second! The model isn't giving any responses? That's because we set `--skip-generation true` to speed up training. We need to turn that back off."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLiq-vuowamh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c93b3c99-00e5-4e05-ce09-01ca6a0d5722"
      },
      "source": [
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    task='empathetic_dialogues',\n",
        "    model_file='from_pretrained/model',\n",
        "    num_examples=2,\n",
        "    skip_generation=False,\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02:33:59 | \u001b[33mOverriding opt[\"skip_generation\"] to False (previously: True)\u001b[0m\n",
            "02:33:59 | Using CUDA\n",
            "02:33:59 | loading dictionary from from_pretrained/model.dict\n",
            "02:33:59 | num words = 54944\n",
            "02:34:00 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "02:34:00 | Loading existing model params from from_pretrained/model\n",
            "02:34:03 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "02:34:03 | Opt:\n",
            "02:34:03 |     activation: gelu\n",
            "02:34:03 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "02:34:03 |     adam_eps: 1e-08\n",
            "02:34:03 |     add_p1_after_newln: False\n",
            "02:34:03 |     aggregate_micro: False\n",
            "02:34:03 |     allow_missing_init_opts: False\n",
            "02:34:03 |     attention_dropout: 0.0\n",
            "02:34:03 |     batchsize: 12\n",
            "02:34:03 |     beam_block_full_context: True\n",
            "02:34:03 |     beam_block_list_filename: None\n",
            "02:34:03 |     beam_block_ngram: -1\n",
            "02:34:03 |     beam_context_block_ngram: -1\n",
            "02:34:03 |     beam_delay: 30\n",
            "02:34:03 |     beam_length_penalty: 0.65\n",
            "02:34:03 |     beam_min_length: 1\n",
            "02:34:03 |     beam_size: 1\n",
            "02:34:03 |     betas: '[0.9, 0.999]'\n",
            "02:34:03 |     bpe_add_prefix_space: None\n",
            "02:34:03 |     bpe_debug: False\n",
            "02:34:03 |     bpe_dropout: None\n",
            "02:34:03 |     bpe_merge: None\n",
            "02:34:03 |     bpe_vocab: None\n",
            "02:34:03 |     compute_tokenized_bleu: False\n",
            "02:34:03 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "02:34:03 |     datatype: train\n",
            "02:34:03 |     delimiter: '\\n'\n",
            "02:34:03 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "02:34:03 |     dict_endtoken: __end__\n",
            "02:34:03 |     dict_file: from_pretrained/model.dict\n",
            "02:34:03 |     dict_include_test: False\n",
            "02:34:03 |     dict_include_valid: False\n",
            "02:34:03 |     dict_initpath: None\n",
            "02:34:03 |     dict_language: english\n",
            "02:34:03 |     dict_loaded: True\n",
            "02:34:03 |     dict_lower: True\n",
            "02:34:03 |     dict_max_ngram_size: -1\n",
            "02:34:03 |     dict_maxexs: -1\n",
            "02:34:03 |     dict_maxtokens: -1\n",
            "02:34:03 |     dict_minfreq: 0\n",
            "02:34:03 |     dict_nulltoken: __null__\n",
            "02:34:03 |     dict_starttoken: __start__\n",
            "02:34:03 |     dict_textfields: text,labels\n",
            "02:34:03 |     dict_tokenizer: bpe\n",
            "02:34:03 |     dict_unktoken: __unk__\n",
            "02:34:03 |     display_add_fields: \n",
            "02:34:03 |     display_examples: False\n",
            "02:34:03 |     download_path: None\n",
            "02:34:03 |     dropout: 0.0\n",
            "02:34:03 |     dynamic_batching: full\n",
            "02:34:03 |     embedding_projection: random\n",
            "02:34:03 |     embedding_size: 512\n",
            "02:34:03 |     embedding_type: random\n",
            "02:34:03 |     embeddings_scale: True\n",
            "02:34:03 |     eval_batchsize: None\n",
            "02:34:03 |     eval_dynamic_batching: None\n",
            "02:34:03 |     evaltask: None\n",
            "02:34:03 |     ffn_size: 2048\n",
            "02:34:03 |     force_fp16_tokens: True\n",
            "02:34:03 |     fp16: True\n",
            "02:34:03 |     fp16_impl: mem_efficient\n",
            "02:34:03 |     gpu: -1\n",
            "02:34:03 |     gradient_clip: 0.1\n",
            "02:34:03 |     hide_labels: False\n",
            "02:34:03 |     history_add_global_end_token: None\n",
            "02:34:03 |     history_reversed: False\n",
            "02:34:03 |     history_size: -1\n",
            "02:34:03 |     image_cropsize: 224\n",
            "02:34:03 |     image_mode: raw\n",
            "02:34:03 |     image_size: 256\n",
            "02:34:03 |     inference: greedy\n",
            "02:34:03 |     init_model: /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "02:34:03 |     init_opt: None\n",
            "02:34:03 |     interactive_mode: False\n",
            "02:34:03 |     invsqrt_lr_decay_gamma: -1\n",
            "02:34:03 |     is_debug: False\n",
            "02:34:03 |     label_truncate: 128\n",
            "02:34:03 |     learn_positional_embeddings: True\n",
            "02:34:03 |     learningrate: 1e-05\n",
            "02:34:03 |     log_every_n_secs: -1\n",
            "02:34:03 |     log_every_n_steps: 50\n",
            "02:34:03 |     loglevel: info\n",
            "02:34:03 |     lr_scheduler: reduceonplateau\n",
            "02:34:03 |     lr_scheduler_decay: 0.5\n",
            "02:34:03 |     lr_scheduler_patience: 3\n",
            "02:34:03 |     max_train_steps: -1\n",
            "02:34:03 |     max_train_time: 600.0\n",
            "02:34:03 |     metrics: default\n",
            "02:34:03 |     model: transformer/generator\n",
            "02:34:03 |     model_file: from_pretrained/model\n",
            "02:34:03 |     model_parallel: False\n",
            "02:34:03 |     momentum: 0\n",
            "02:34:03 |     multitask_weights: [1]\n",
            "02:34:03 |     mutators: None\n",
            "02:34:03 |     n_decoder_layers: -1\n",
            "02:34:03 |     n_encoder_layers: -1\n",
            "02:34:03 |     n_heads: 16\n",
            "02:34:03 |     n_layers: 8\n",
            "02:34:03 |     n_positions: 512\n",
            "02:34:03 |     n_segments: 0\n",
            "02:34:03 |     nesterov: True\n",
            "02:34:03 |     no_cuda: False\n",
            "02:34:03 |     num_epochs: -1\n",
            "02:34:03 |     num_examples: 2\n",
            "02:34:03 |     num_workers: 0\n",
            "02:34:03 |     nus: [0.7]\n",
            "02:34:03 |     optimizer: mem_eff_adam\n",
            "02:34:03 |     output_scaling: 1.0\n",
            "02:34:03 |     override: \"{'task': 'empathetic_dialogues', 'model_file': 'from_pretrained/model', 'num_examples': '2', 'skip_generation': False}\"\n",
            "02:34:03 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "02:34:03 |     person_tokens: False\n",
            "02:34:03 |     rank_candidates: False\n",
            "02:34:03 |     relu_dropout: 0.0\n",
            "02:34:03 |     remove_political_convos: False\n",
            "02:34:03 |     save_after_valid: False\n",
            "02:34:03 |     save_every_n_secs: -1\n",
            "02:34:03 |     share_word_embeddings: True\n",
            "02:34:03 |     short_final_eval: False\n",
            "02:34:03 |     skip_generation: False\n",
            "02:34:03 |     special_tok_lst: None\n",
            "02:34:03 |     split_lines: False\n",
            "02:34:03 |     starttime: Jul21_02-20\n",
            "02:34:03 |     task: empathetic_dialogues\n",
            "02:34:03 |     temperature: 1.0\n",
            "02:34:03 |     tensorboard_log: False\n",
            "02:34:03 |     tensorboard_logdir: None\n",
            "02:34:03 |     text_truncate: 512\n",
            "02:34:03 |     topk: 10\n",
            "02:34:03 |     topp: 0.9\n",
            "02:34:03 |     train_experiencer_only: False\n",
            "02:34:03 |     truncate: -1\n",
            "02:34:03 |     update_freq: 1\n",
            "02:34:03 |     use_reply: label\n",
            "02:34:03 |     validation_cutoff: 1.0\n",
            "02:34:03 |     validation_every_n_epochs: 0.25\n",
            "02:34:03 |     validation_every_n_secs: -1\n",
            "02:34:03 |     validation_every_n_steps: -1\n",
            "02:34:03 |     validation_max_exs: -1\n",
            "02:34:03 |     validation_metric: ppl\n",
            "02:34:03 |     validation_metric_mode: None\n",
            "02:34:03 |     validation_patience: 10\n",
            "02:34:03 |     validation_share_agent: False\n",
            "02:34:03 |     variant: xlm\n",
            "02:34:03 |     verbose: False\n",
            "02:34:03 |     wandb_entity: None\n",
            "02:34:03 |     wandb_log: False\n",
            "02:34:03 |     wandb_name: None\n",
            "02:34:03 |     wandb_project: None\n",
            "02:34:03 |     warmup_rate: 0.0001\n",
            "02:34:03 |     warmup_updates: 100\n",
            "02:34:03 |     weight_decay: None\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues- - -\u001b[0;0m\n",
            "\u001b[0mToday,as i was leaving for work in the morning,i had a tire burst in the middle of a busy road. That scared the hell out of me!\u001b[0;0m\n",
            "\u001b[1;94m    labels: Are you fine now?\u001b[0;0m\n",
            "\u001b[0;95m     model: oh no ! that sounds scary ! did you get a new tire ?\u001b[0;0m\n",
            "\u001b[0mYeah,i'm doing alright now, but with minor injuries.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Cool :) Is your car damaged a lot?\u001b[0;0m\n",
            "\u001b[0;95m     model: that ' s good to hear . i hope you ' re okay .\u001b[0;0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MR0rn0ZwyxQ"
      },
      "source": [
        "On the command line:\n",
        "```bash\n",
        "python -m parlai.scripts.display_model --task empathetic_dialogues --model-file zoo:tutorial_transformer_generator/model\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYuaSPWrw0Il"
      },
      "source": [
        "# Bringing your own datasets\n",
        "\n",
        "What if you want to build your own dataset in ParlAI? Of course you can do that!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SgJi8XHwtph",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a13ab86-3cc1-4727-be01-af1410f35140"
      },
      "source": [
        "from parlai.core.teachers import register_teacher, DialogTeacher\n",
        "\n",
        "@register_teacher(\"my_teacher\")\n",
        "class MyTeacher(DialogTeacher):\n",
        "    def __init__(self, opt, shared=None):\n",
        "        # opt is the command line arguments.\n",
        "        \n",
        "        # What is this shared thing?\n",
        "        # We make many copies of a teacher, one-per-batchsize. Shared lets us store \n",
        "        \n",
        "        # We just need to set the \"datafile\".  This is boilerplate, but differs in many teachers.\n",
        "        # The \"datafile\" is the filename where we will load the data from. In this case, we'll set it to\n",
        "        # the fold name (train/valid/test) + \".txt\"\n",
        "        opt['datafile'] = opt['datatype'].split(':')[0] + \".txt\"\n",
        "        super().__init__(opt, shared)\n",
        "    \n",
        "    def setup_data(self, datafile):\n",
        "        # filename tells us where to load from.\n",
        "        # We'll just use some hardcoded data, but show how you could read the filename here:\n",
        "        print(f\" ~~ Loading from {datafile} ~~ \")\n",
        "        \n",
        "        # setup_data should yield tuples of ((text, label), new_episode)\n",
        "        # That is ((str, str), bool)\n",
        "        \n",
        "        # first episode\n",
        "        # notice how we have call, response, and then True? The True indicates this is a first message\n",
        "        # in a conversation\n",
        "        yield ('Hello', 'Hi'), True\n",
        "        # Next we have the second turn. This time, the last element is False, indicating we're still going\n",
        "        yield ('How are you', 'I am fine'), False\n",
        "        yield (\"Let's say goodbye\", 'Goodbye!'), False\n",
        "        \n",
        "        # second episode. We need to have True again!\n",
        "        yield (\"Hey\", \"hi there\"), True\n",
        "        yield (\"Deja vu?\", \"Deja vu!\"), False\n",
        "        yield (\"Last chance\", \"This is it\"), False\n",
        "        \n",
        "        \n",
        "DisplayData.main(task=\"my_teacher\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02:39:47 | Opt:\n",
            "02:39:47 |     allow_missing_init_opts: False\n",
            "02:39:47 |     batchsize: 1\n",
            "02:39:47 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "02:39:47 |     datatype: train:ordered\n",
            "02:39:47 |     dict_class: None\n",
            "02:39:47 |     display_add_fields: \n",
            "02:39:47 |     download_path: None\n",
            "02:39:47 |     dynamic_batching: None\n",
            "02:39:47 |     hide_labels: False\n",
            "02:39:47 |     ignore_agent_reply: True\n",
            "02:39:47 |     image_cropsize: 224\n",
            "02:39:47 |     image_mode: raw\n",
            "02:39:47 |     image_size: 256\n",
            "02:39:47 |     init_model: None\n",
            "02:39:47 |     init_opt: None\n",
            "02:39:47 |     is_debug: False\n",
            "02:39:47 |     loglevel: info\n",
            "02:39:47 |     max_display_len: 1000\n",
            "02:39:47 |     model: None\n",
            "02:39:47 |     model_file: None\n",
            "02:39:47 |     multitask_weights: [1]\n",
            "02:39:47 |     mutators: None\n",
            "02:39:47 |     num_examples: 10\n",
            "02:39:47 |     override: \"{'task': 'my_teacher'}\"\n",
            "02:39:47 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "02:39:47 |     starttime: Jul21_02-39\n",
            "02:39:47 |     task: my_teacher\n",
            "02:39:47 |     verbose: False\n",
            "02:39:47 | creating task(s): my_teacher\n",
            " ~~ Loading from train.txt ~~ \n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher - - -\u001b[0;0m\n",
            "\u001b[0mHello\u001b[0;0m\n",
            "   \u001b[1;94mHi\u001b[0;0m\n",
            "\u001b[0mHow are you\u001b[0;0m\n",
            "   \u001b[1;94mI am fine\u001b[0;0m\n",
            "\u001b[0mLet's say goodbye\u001b[0;0m\n",
            "   \u001b[1;94mGoodbye!\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher - - -\u001b[0;0m\n",
            "\u001b[0mHey\u001b[0;0m\n",
            "   \u001b[1;94mhi there\u001b[0;0m\n",
            "\u001b[0mDeja vu?\u001b[0;0m\n",
            "   \u001b[1;94mDeja vu!\u001b[0;0m\n",
            "\u001b[0mLast chance\u001b[0;0m\n",
            "   \u001b[1;94mThis is it\u001b[0;0m\n",
            "02:39:47 | epoch done\n",
            "02:39:47 | loaded 2 episodes with a total of 6 examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vvwxi6gXw8jU"
      },
      "source": [
        "Notice how the data corresponds to the utterances we provided? In reality, we'd normally want to load up a data file, loop through it, and yield the tuples from processed data. But for this simple example, it works well.\n",
        "\n",
        "We can now use our teacher in the standard places! Let's see how the model we trained earlier behaves with it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIyZQnxAw5HG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10c335c3-e50b-457e-d8b2-f83cbb97c4b3"
      },
      "source": [
        "DisplayModel.main(task='my_teacher', model_file='from_pretrained/model', skip_generation=False)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02:40:37 | \u001b[33mOverriding opt[\"task\"] to my_teacher (previously: empathetic_dialogues)\u001b[0m\n",
            "02:40:37 | \u001b[33mOverriding opt[\"skip_generation\"] to False (previously: True)\u001b[0m\n",
            "02:40:37 | Using CUDA\n",
            "02:40:37 | loading dictionary from from_pretrained/model.dict\n",
            "02:40:38 | num words = 54944\n",
            "02:40:39 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "02:40:39 | Loading existing model params from from_pretrained/model\n",
            "02:40:42 | creating task(s): my_teacher\n",
            " ~~ Loading from valid.txt ~~ \n",
            "02:40:42 | Opt:\n",
            "02:40:42 |     activation: gelu\n",
            "02:40:42 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "02:40:42 |     adam_eps: 1e-08\n",
            "02:40:42 |     add_p1_after_newln: False\n",
            "02:40:42 |     aggregate_micro: False\n",
            "02:40:42 |     allow_missing_init_opts: False\n",
            "02:40:42 |     attention_dropout: 0.0\n",
            "02:40:42 |     batchsize: 12\n",
            "02:40:42 |     beam_block_full_context: True\n",
            "02:40:42 |     beam_block_list_filename: None\n",
            "02:40:42 |     beam_block_ngram: -1\n",
            "02:40:42 |     beam_context_block_ngram: -1\n",
            "02:40:42 |     beam_delay: 30\n",
            "02:40:42 |     beam_length_penalty: 0.65\n",
            "02:40:42 |     beam_min_length: 1\n",
            "02:40:42 |     beam_size: 1\n",
            "02:40:42 |     betas: '[0.9, 0.999]'\n",
            "02:40:42 |     bpe_add_prefix_space: None\n",
            "02:40:42 |     bpe_debug: False\n",
            "02:40:42 |     bpe_dropout: None\n",
            "02:40:42 |     bpe_merge: None\n",
            "02:40:42 |     bpe_vocab: None\n",
            "02:40:42 |     compute_tokenized_bleu: False\n",
            "02:40:42 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "02:40:42 |     datatype: train\n",
            "02:40:42 |     delimiter: '\\n'\n",
            "02:40:42 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "02:40:42 |     dict_endtoken: __end__\n",
            "02:40:42 |     dict_file: from_pretrained/model.dict\n",
            "02:40:42 |     dict_include_test: False\n",
            "02:40:42 |     dict_include_valid: False\n",
            "02:40:42 |     dict_initpath: None\n",
            "02:40:42 |     dict_language: english\n",
            "02:40:42 |     dict_loaded: True\n",
            "02:40:42 |     dict_lower: True\n",
            "02:40:42 |     dict_max_ngram_size: -1\n",
            "02:40:42 |     dict_maxexs: -1\n",
            "02:40:42 |     dict_maxtokens: -1\n",
            "02:40:42 |     dict_minfreq: 0\n",
            "02:40:42 |     dict_nulltoken: __null__\n",
            "02:40:42 |     dict_starttoken: __start__\n",
            "02:40:42 |     dict_textfields: text,labels\n",
            "02:40:42 |     dict_tokenizer: bpe\n",
            "02:40:42 |     dict_unktoken: __unk__\n",
            "02:40:42 |     display_add_fields: \n",
            "02:40:42 |     display_examples: False\n",
            "02:40:42 |     download_path: None\n",
            "02:40:42 |     dropout: 0.0\n",
            "02:40:42 |     dynamic_batching: full\n",
            "02:40:42 |     embedding_projection: random\n",
            "02:40:42 |     embedding_size: 512\n",
            "02:40:42 |     embedding_type: random\n",
            "02:40:42 |     embeddings_scale: True\n",
            "02:40:42 |     eval_batchsize: None\n",
            "02:40:42 |     eval_dynamic_batching: None\n",
            "02:40:42 |     evaltask: None\n",
            "02:40:42 |     ffn_size: 2048\n",
            "02:40:42 |     force_fp16_tokens: True\n",
            "02:40:42 |     fp16: True\n",
            "02:40:42 |     fp16_impl: mem_efficient\n",
            "02:40:42 |     gpu: -1\n",
            "02:40:42 |     gradient_clip: 0.1\n",
            "02:40:42 |     hide_labels: False\n",
            "02:40:42 |     history_add_global_end_token: None\n",
            "02:40:42 |     history_reversed: False\n",
            "02:40:42 |     history_size: -1\n",
            "02:40:42 |     image_cropsize: 224\n",
            "02:40:42 |     image_mode: raw\n",
            "02:40:42 |     image_size: 256\n",
            "02:40:42 |     inference: greedy\n",
            "02:40:42 |     init_model: /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "02:40:42 |     init_opt: None\n",
            "02:40:42 |     interactive_mode: False\n",
            "02:40:42 |     invsqrt_lr_decay_gamma: -1\n",
            "02:40:42 |     is_debug: False\n",
            "02:40:42 |     label_truncate: 128\n",
            "02:40:42 |     learn_positional_embeddings: True\n",
            "02:40:42 |     learningrate: 1e-05\n",
            "02:40:42 |     log_every_n_secs: -1\n",
            "02:40:42 |     log_every_n_steps: 50\n",
            "02:40:42 |     loglevel: info\n",
            "02:40:42 |     lr_scheduler: reduceonplateau\n",
            "02:40:42 |     lr_scheduler_decay: 0.5\n",
            "02:40:42 |     lr_scheduler_patience: 3\n",
            "02:40:42 |     max_train_steps: -1\n",
            "02:40:42 |     max_train_time: 600.0\n",
            "02:40:42 |     metrics: default\n",
            "02:40:42 |     model: transformer/generator\n",
            "02:40:42 |     model_file: from_pretrained/model\n",
            "02:40:42 |     model_parallel: False\n",
            "02:40:42 |     momentum: 0\n",
            "02:40:42 |     multitask_weights: [1]\n",
            "02:40:42 |     mutators: None\n",
            "02:40:42 |     n_decoder_layers: -1\n",
            "02:40:42 |     n_encoder_layers: -1\n",
            "02:40:42 |     n_heads: 16\n",
            "02:40:42 |     n_layers: 8\n",
            "02:40:42 |     n_positions: 512\n",
            "02:40:42 |     n_segments: 0\n",
            "02:40:42 |     nesterov: True\n",
            "02:40:42 |     no_cuda: False\n",
            "02:40:42 |     num_epochs: -1\n",
            "02:40:42 |     num_examples: 10\n",
            "02:40:42 |     num_workers: 0\n",
            "02:40:42 |     nus: [0.7]\n",
            "02:40:42 |     optimizer: mem_eff_adam\n",
            "02:40:42 |     output_scaling: 1.0\n",
            "02:40:42 |     override: \"{'task': 'my_teacher', 'model_file': 'from_pretrained/model', 'skip_generation': False}\"\n",
            "02:40:42 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "02:40:42 |     person_tokens: False\n",
            "02:40:42 |     rank_candidates: False\n",
            "02:40:42 |     relu_dropout: 0.0\n",
            "02:40:42 |     remove_political_convos: False\n",
            "02:40:42 |     save_after_valid: False\n",
            "02:40:42 |     save_every_n_secs: -1\n",
            "02:40:42 |     share_word_embeddings: True\n",
            "02:40:42 |     short_final_eval: False\n",
            "02:40:42 |     skip_generation: False\n",
            "02:40:42 |     special_tok_lst: None\n",
            "02:40:42 |     split_lines: False\n",
            "02:40:42 |     starttime: Jul21_02-20\n",
            "02:40:42 |     task: my_teacher\n",
            "02:40:42 |     temperature: 1.0\n",
            "02:40:42 |     tensorboard_log: False\n",
            "02:40:42 |     tensorboard_logdir: None\n",
            "02:40:42 |     text_truncate: 512\n",
            "02:40:42 |     topk: 10\n",
            "02:40:42 |     topp: 0.9\n",
            "02:40:42 |     train_experiencer_only: False\n",
            "02:40:42 |     truncate: -1\n",
            "02:40:42 |     update_freq: 1\n",
            "02:40:42 |     use_reply: label\n",
            "02:40:42 |     validation_cutoff: 1.0\n",
            "02:40:42 |     validation_every_n_epochs: 0.25\n",
            "02:40:42 |     validation_every_n_secs: -1\n",
            "02:40:42 |     validation_every_n_steps: -1\n",
            "02:40:42 |     validation_max_exs: -1\n",
            "02:40:42 |     validation_metric: ppl\n",
            "02:40:42 |     validation_metric_mode: None\n",
            "02:40:42 |     validation_patience: 10\n",
            "02:40:42 |     validation_share_agent: False\n",
            "02:40:42 |     variant: xlm\n",
            "02:40:42 |     verbose: False\n",
            "02:40:42 |     wandb_entity: None\n",
            "02:40:42 |     wandb_log: False\n",
            "02:40:42 |     wandb_name: None\n",
            "02:40:42 |     wandb_project: None\n",
            "02:40:42 |     warmup_rate: 0.0001\n",
            "02:40:42 |     warmup_updates: 100\n",
            "02:40:42 |     weight_decay: None\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHello\u001b[0;0m\n",
            "\u001b[1;94m    labels: Hi\u001b[0;0m\n",
            "\u001b[0;95m     model: hi\u001b[0;0m\n",
            "\u001b[0mHow are you\u001b[0;0m\n",
            "\u001b[1;94m    labels: I am fine\u001b[0;0m\n",
            "\u001b[0;95m     model: i ' m good , how are you ?\u001b[0;0m\n",
            "\u001b[0mLet's say goodbye\u001b[0;0m\n",
            "\u001b[1;94m    labels: Goodbye!\u001b[0;0m\n",
            "\u001b[0;95m     model: i am fine\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHey\u001b[0;0m\n",
            "\u001b[1;94m    labels: hi there\u001b[0;0m\n",
            "\u001b[0;95m     model: hey , i ' m here to help you out .\u001b[0;0m\n",
            "\u001b[0mDeja vu?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Deja vu!\u001b[0;0m\n",
            "\u001b[0;95m     model: i ' ve just been in this place before\u001b[0;0m\n",
            "\u001b[0mLast chance\u001b[0;0m\n",
            "\u001b[1;94m    labels: This is it\u001b[0;0m\n",
            "\u001b[0;95m     model: i ' ve just been in this place before\u001b[0;0m\n",
            "02:40:43 | epoch done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOzvSHAy0meK"
      },
      "source": [
        "Note that the `register_teacher` decorator makes the commands aware of your teacher. If you leave it off, the commands won't be able to locate it. If you want to use your teacher on the command line, you'll need to put it in a very specific filename: `parlai/agents/my_teacher/agents.py`, and you'll need to name the class `DefaultTeacher` instead of `MyTeacher`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJj7Lhs00oOB"
      },
      "source": [
        "# Creating your own models\n",
        "\n",
        "As a start, we'll implement a *very* simple agent. This agent will just sort of respond with \"hello X, my name is Y\", where X is based on the input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pykhtFDrxCPo"
      },
      "source": [
        "from parlai.core.agents import register_agent, Agent\n",
        "\n",
        "@register_agent(\"hello\")\n",
        "class HelloAgent(Agent):\n",
        "    @classmethod\n",
        "    def add_cmdline_args(cls, parser):\n",
        "        parser.add_argument('--name', type=str, default='Alice', help=\"The agent's name.\")\n",
        "        return parser\n",
        "        \n",
        "    def __init__(self, opt, shared=None):\n",
        "        # similar to the teacher, we have the Opt and the shared memory objects!\n",
        "        super().__init__(opt, shared)\n",
        "        self.id = 'HelloAgent'\n",
        "        self.name = opt['name']\n",
        "    \n",
        "    def observe(self, observation):\n",
        "        # Gather the last word from the other user's input\n",
        "        words = observation.get('text', '').split()\n",
        "        if words:\n",
        "            self.last_word = words[-1]\n",
        "        else:\n",
        "            self.last_word = \"stranger!\"\n",
        "    \n",
        "    def act(self):\n",
        "        # Always return a string like this.\n",
        "        return {\n",
        "            'id': self.id,\n",
        "            'text': f\"Hello {self.last_word}, I'm {self.name}\",\n",
        "        }"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1SZmy_s0sGd"
      },
      "source": [
        "Let's try seeing how this agent behaves:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcS1UIFH0pb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "11f56f07-fb0c-48cd-c9de-5548c4813d0c"
      },
      "source": [
        "DisplayModel.main(task='my_teacher', model='hello')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/params.py\u001b[0m in \u001b[0;36madd_model_subargs\u001b[0;34m(self, model, partial)\u001b[0m\n\u001b[1;32m    824\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'add_cmdline_args'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_cmdline_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: add_cmdline_args() takes 2 positional arguments but 3 were given",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-e7c0e0369f4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mDisplayModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'my_teacher'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hello'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/script.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/script.py\u001b[0m in \u001b[0;36m_run_kwargs\u001b[0;34m(cls, kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \"\"\"\n\u001b[1;32m     90\u001b[0m         \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_from_parser_and_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/params.py\u001b[0m in \u001b[0;36mparse_kwargs\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_captured_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m             \u001b[0mstring_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs_to_str_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstring_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/params.py\u001b[0m in \u001b[0;36m_kwargs_to_str_args\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1226\u001b[0m         \u001b[0;31m# become aware of any extra args that might be specified if the user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0;31m# provides something like model=\"transformer/generator\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1228\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_extra_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m         \u001b[0;31m# do it again, this time knowing about ALL args.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/params.py\u001b[0m in \u001b[0;36madd_extra_args\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    946\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_model_subargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m         \u001b[0;31m# add world args, if we know a priori which world is being used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/params.py\u001b[0m in \u001b[0;36madd_model_subargs\u001b[0;34m(self, model, partial)\u001b[0m\n\u001b[1;32m    830\u001b[0m                 \u001b[0;34m\"to add_cmdline_args(argparser, partial_opt). For details, see \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m                 \u001b[0;34m\"https://github.com/facebookresearch/ParlAI/pull/3328.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m             ) from typ\n\u001b[0m\u001b[1;32m    833\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgumentError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;31m# already added\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Agent 'hello' appears to have signature add_cmdline_args(argparser) but we have updated the signature to add_cmdline_args(argparser, partial_opt). For details, see https://github.com/facebookresearch/ParlAI/pull/3328."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmvcRSGS0wQE"
      },
      "source": [
        "Notice how it read the words from the user, and provides its name from the command line argument? We can also interact with it easily enough."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xd5CaG00tv6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "4d9aee8c-d390-46c7-e685-febcd5a5b91c"
      },
      "source": [
        "Interactive.main(model='hello', name='Bob')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ optional arguments: ] \n",
            "[  display_examples: False ]\n",
            "[  display_ignore_fields: label_candidates,text_candidates ]\n",
            "[  display_prettify: False ]\n",
            "[  interactive_task: True ]\n",
            "[  name: Bob ]\n",
            "[ Main ParlAI Arguments: ] \n",
            "[  batchsize: 1 ]\n",
            "[  datapath: /usr/local/lib/python3.6/dist-packages/data ]\n",
            "[  datatype: train ]\n",
            "[  download_path: /usr/local/lib/python3.6/dist-packages/downloads ]\n",
            "[  dynamic_batching: None ]\n",
            "[  hide_labels: False ]\n",
            "[  image_mode: raw ]\n",
            "[  init_opt: None ]\n",
            "[  multitask_weights: [1] ]\n",
            "[  numthreads: 1 ]\n",
            "[  show_advanced_args: False ]\n",
            "[  task: interactive ]\n",
            "[ ParlAI Model Arguments: ] \n",
            "[  dict_class: None ]\n",
            "[  init_model: None ]\n",
            "[  model: hello ]\n",
            "[  model_file: None ]\n",
            "[ Local Human Arguments: ] \n",
            "[  local_human_candidates_file: None ]\n",
            "[  single_turn: False ]\n",
            "[ ParlAI Image Preprocessing Arguments: ] \n",
            "[  image_cropsize: 224 ]\n",
            "[  image_size: 256 ]\n",
            "\u001b[1;31mEnter [DONE] if you want to end the episode, [EXIT] to quit.\u001b[0;0m\n",
            "[creating task(s): interactive]\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m hi, who are you?\n",
            "\u001b[0;34m[HelloAgent]:\u001b[0;0m \u001b[1mHello you?, I'm Bob\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m My name is Stephen\n",
            "\u001b[0;34m[HelloAgent]:\u001b[0;0m \u001b[1mHello Stephen, I'm Bob\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m [EXIT]\n",
            "\u001b[0;34m[HelloAgent]:\u001b[0;0m \u001b[1mHello stranger!, I'm Bob\u001b[0;0m\n",
            "CHAT DONE \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAe1hytf1BPk"
      },
      "source": [
        "Similar to the teacher, the call to `register_agent` makes it available for use in commands. If you forget the `register_agent` decorator, you won't be able to refer to it. Similarly, if you wanted to use this model from the command line, you would need to save this code to a special folder: `parlai/agents/hello/hello.py`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aBbhKTO1DEE"
      },
      "source": [
        "## Creating a neural network model\n",
        "\n",
        "The base Agent class is very simple, but it also provides extremely little functionality. We have created solid abstractions for creating neural-network type models. [`TorchGeneratorAgent`](https://parl.ai/docs/torch_agent.html#module-parlai.core.torch_generator_agent) is one our common abstractions, and it assumes a model which outputs one-word-at-a-time.\n",
        "\n",
        "The following is from our [ExampleSeq2Seq](https://github.com/facebookresearch/ParlAI/blob/master/parlai/agents/examples/seq2seq.py) agent. It's a simple RNN model, trained like a Machine Translation model. The Model is too complex to go over in this document, but please feel free to [read our TorchGeneratorAgent tutorial](https://parl.ai/docs/tutorial_torch_generator_agent.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVrZh-T903wh"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import parlai.core.torch_generator_agent as tga\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Example encoder, consisting of an embedding layer and a 1-layer LSTM with the\n",
        "    specified hidden size.\n",
        "    Pay particular attention to the ``forward`` output.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embeddings, hidden_size):\n",
        "        \"\"\"\n",
        "        Initialization.\n",
        "        Arguments here can be used to provide hyperparameters.\n",
        "        \"\"\"\n",
        "        # must call super on all nn.Modules.\n",
        "        super().__init__()\n",
        "\n",
        "        self.embeddings = embeddings\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=hidden_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "        )\n",
        "\n",
        "    def forward(self, input_tokens):\n",
        "        \"\"\"\n",
        "        Perform the forward pass for the encoder.\n",
        "        Input *must* be input_tokens, which are the context tokens given\n",
        "        as a matrix of lookup IDs.\n",
        "        :param input_tokens:\n",
        "            Input tokens as a bsz x seqlen LongTensor.\n",
        "            Likely will contain padding.\n",
        "        :return:\n",
        "            You can return anything you like; it is will be passed verbatim\n",
        "            into the decoder for conditioning. However, it should be something\n",
        "            you can easily manipulate in ``reorder_encoder_states``.\n",
        "            This particular implementation returns the hidden and cell states from the\n",
        "            LSTM.\n",
        "        \"\"\"\n",
        "        embedded = self.embeddings(input_tokens)\n",
        "        _output, hidden = self.lstm(embedded)\n",
        "        return hidden\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic example decoder, consisting of an embedding layer and a 1-layer LSTM with the\n",
        "    specified hidden size. Decoder allows for incremental decoding by ingesting the\n",
        "    current incremental state on each forward pass.\n",
        "    Pay particular note to the ``forward``.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embeddings, hidden_size):\n",
        "        \"\"\"\n",
        "        Initialization.\n",
        "        Arguments here can be used to provide hyperparameters.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.embeddings = embeddings\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=hidden_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "        )\n",
        "\n",
        "    def forward(self, input, encoder_state, incr_state=None):\n",
        "        \"\"\"\n",
        "        Run forward pass.\n",
        "        :param input:\n",
        "            The currently generated tokens from the decoder.\n",
        "        :param encoder_state:\n",
        "            The output from the encoder module.\n",
        "        :parm incr_state:\n",
        "            The previous hidden state of the decoder.\n",
        "        \"\"\"\n",
        "        embedded = self.embeddings(input)\n",
        "        if incr_state is None:\n",
        "            # this is our very first call. We want to seed the LSTM with the\n",
        "            # hidden state of the decoder\n",
        "            state = encoder_state\n",
        "        else:\n",
        "            # We've generated some tokens already, so we can reuse the existing\n",
        "            # decoder state\n",
        "            state = incr_state\n",
        "\n",
        "        # get the new output and decoder incremental state\n",
        "        output, incr_state = self.lstm(embedded, state)\n",
        "\n",
        "        return output, incr_state\n",
        "\n",
        "\n",
        "class ExampleModel(tga.TorchGeneratorModel):\n",
        "    \"\"\"\n",
        "    ExampleModel implements the abstract methods of TorchGeneratorModel to define how to\n",
        "    re-order encoder states and decoder incremental states.\n",
        "    It also instantiates the embedding table, encoder, and decoder, and defines the\n",
        "    final output layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dictionary, hidden_size=1024):\n",
        "        super().__init__(\n",
        "            padding_idx=dictionary[dictionary.null_token],\n",
        "            start_idx=dictionary[dictionary.start_token],\n",
        "            end_idx=dictionary[dictionary.end_token],\n",
        "            unknown_idx=dictionary[dictionary.unk_token],\n",
        "        )\n",
        "        self.embeddings = nn.Embedding(len(dictionary), hidden_size)\n",
        "        self.encoder = Encoder(self.embeddings, hidden_size)\n",
        "        self.decoder = Decoder(self.embeddings, hidden_size)\n",
        "\n",
        "    def output(self, decoder_output):\n",
        "        \"\"\"\n",
        "        Perform the final output -> logits transformation.\n",
        "        \"\"\"\n",
        "        return F.linear(decoder_output, self.embeddings.weight)\n",
        "\n",
        "    def reorder_encoder_states(self, encoder_states, indices):\n",
        "        \"\"\"\n",
        "        Reorder the encoder states to select only the given batch indices.\n",
        "        Since encoder_state can be arbitrary, you must implement this yourself.\n",
        "        Typically you will just want to index select on the batch dimension.\n",
        "        \"\"\"\n",
        "        h, c = encoder_states\n",
        "        return h[:, indices, :], c[:, indices, :]\n",
        "\n",
        "    def reorder_decoder_incremental_state(self, incr_state, indices):\n",
        "        \"\"\"\n",
        "        Reorder the decoder states to select only the given batch indices.\n",
        "        This method can be a stub which always returns None; this will result in the\n",
        "        decoder doing a complete forward pass for every single token, making generation\n",
        "        O(n^2). However, if any state can be cached, then this method should be\n",
        "        implemented to reduce the generation complexity to O(n).\n",
        "        \"\"\"\n",
        "        h, c = incr_state\n",
        "        return h[:, indices, :], c[:, indices, :]\n",
        "\n",
        "\n",
        "@register_agent(\"my_first_lstm\")\n",
        "class Seq2seqAgent(tga.TorchGeneratorAgent):\n",
        "    \"\"\"\n",
        "    Example agent.\n",
        "    Implements the interface for TorchGeneratorAgent. The minimum requirement is that it\n",
        "    implements ``build_model``, but we will want to include additional command line\n",
        "    parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def add_cmdline_args(cls, argparser):\n",
        "        \"\"\"\n",
        "        Add CLI arguments.\n",
        "        \"\"\"\n",
        "        # Make sure to add all of TorchGeneratorAgent's arguments\n",
        "        super(Seq2seqAgent, cls).add_cmdline_args(argparser)\n",
        "\n",
        "        # Add custom arguments only for this model.\n",
        "        group = argparser.add_argument_group('Example TGA Agent')\n",
        "        group.add_argument(\n",
        "            '-hid', '--hidden-size', type=int, default=1024, help='Hidden size.'\n",
        "        )\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"\n",
        "        Construct the model.\n",
        "        \"\"\"\n",
        "\n",
        "        model = ExampleModel(self.dict, self.opt['hidden_size'])\n",
        "        # Optionally initialize pre-trained embeddings by copying them from another\n",
        "        # source: GloVe, fastText, etc.\n",
        "        self._copy_embeddings(model.embeddings.weight, self.opt['embedding_type'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfR9w_Hm1HHY"
      },
      "source": [
        "Of course, now we can train with our new model. Let's train it on our toy task that we created earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJMXpogz1E-_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "outputId": "3fe8275b-a7cd-491a-a761-fbd5849e2a7c"
      },
      "source": [
        "# of course, we can train the model! Let's Train it on our silly toy task from above\n",
        "!rm -rf my_first_lstm\n",
        "!mkdir -p my_first_lstm\n",
        "\n",
        "TrainModel.main(\n",
        "    model='my_first_lstm',\n",
        "    model_file='my_first_lstm/model',\n",
        "    task='my_teacher',\n",
        "    batchsize=1,\n",
        "    validation_every_n_secs=10,\n",
        "    max_train_time=60,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building dictionary: 100%|██████████| 6.00/6.00 [00:00<00:00, 1.91kex/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[ building dictionary first... ]\n",
            "[creating task(s): my_teacher]\n",
            " ~~ Loading from train.txt ~~ \n",
            " ~~ Loading from train.txt ~~ \n",
            "Dictionary: saving dictionary to my_first_lstm/model.dict\n",
            "[ dictionary built with 30 tokens in 0s ]\n",
            "[ no model with opt yet at: my_first_lstm/model(.opt) ]\n",
            "[ Using CUDA ]\n",
            "Dictionary: loading dictionary from my_first_lstm/model.dict\n",
            "[ num words =  30 ]\n",
            "Total parameters: 16,824,320 (16,824,320 trainable)\n",
            "[creating task(s): my_teacher]\n",
            " ~~ Loading from train.txt ~~ \n",
            "[ training... ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[ time:10.0s total_exs:1828 epochs:304.67 ]\n",
            "     clip  exs  gnorm  gpu_mem   loss  lr   ppl  token_acc  total_train_updates   tpb  updates\n",
            "   .01641 1828  1.368    .9171 .04105   1 1.042      .9942                 1828 3.328     1828\n",
            "\n",
            "[ time:10.0s total_exs:1828 epochs:304.67 ]\n",
            "    gpu_mem  lr  total_train_updates\n",
            "      .9171   1                 1828\n",
            "\n",
            "[creating task(s): my_teacher]\n",
            " ~~ Loading from valid.txt ~~ \n",
            "[ running eval: valid ]\n",
            "[ eval completed in 0.06s ]\n",
            "valid:\n",
            "    accuracy   bleu-4  exs  f1  gpu_mem  loss  lr  ppl  token_acc  total_train_updates   tpb\n",
            "           1 .0003337    6   1    .9171     0   1    1          1                 1828 3.333\n",
            "\n",
            "[ new best accuracy: 1 ]\n",
            "[ saving best valid model: my_first_lstm/model ]\n",
            "[ task solved! stopping. ]\n",
            "[ Using CUDA ]\n",
            "Dictionary: loading dictionary from my_first_lstm/model.dict\n",
            "[ num words =  30 ]\n",
            "Total parameters: 16,824,320 (16,824,320 trainable)\n",
            "[ Loading existing model params from my_first_lstm/model ]\n",
            "[creating task(s): my_teacher]\n",
            " ~~ Loading from valid.txt ~~ \n",
            "[ running eval: valid ]\n",
            "[ eval completed in 0.05s ]\n",
            "valid:\n",
            "    accuracy   bleu-4  exs  f1  gpu_mem  loss  lr  ppl  token_acc  total_train_updates   tpb\n",
            "           1 .0003337    6   1    .9131     0   1    1          1                 1828 3.333\n",
            "\n",
            "[creating task(s): my_teacher]\n",
            " ~~ Loading from test.txt ~~ \n",
            "[ running eval: test ]\n",
            "[ eval completed in 0.05s ]\n",
            "test:\n",
            "    accuracy   bleu-4  exs  f1  gpu_mem  loss  lr  ppl  token_acc  total_train_updates   tpb\n",
            "           1 .0003337    6   1    .9131     0   1    1          1                 1828 3.333\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hHrruVd1KnK"
      },
      "source": [
        "Let's see how it does. It should reproduce the data perfectly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shqFpdrE1Iif",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "1a89571c-2a42-48b8-9752-dffc4a58e557"
      },
      "source": [
        "DisplayModel.main(model_file='my_first_lstm/model', task='my_teacher')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ Using CUDA ]\n",
            "Dictionary: loading dictionary from my_first_lstm/model.dict\n",
            "[ num words =  30 ]\n",
            "Total parameters: 16,824,320 (16,824,320 trainable)\n",
            "[ Loading existing model params from my_first_lstm/model ]\n",
            "[creating task(s): my_teacher]\n",
            " ~~ Loading from valid.txt ~~ \n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHello\u001b[0;0m\n",
            "\u001b[1;94m    labels: Hi\u001b[0;0m\n",
            "\u001b[0;95m     model: Hi\u001b[0;0m\n",
            "\u001b[0mHow are you\u001b[0;0m\n",
            "\u001b[1;94m    labels: I am fine\u001b[0;0m\n",
            "\u001b[0;95m     model: I am fine\u001b[0;0m\n",
            "\u001b[0mLet's say goodbye\u001b[0;0m\n",
            "\u001b[1;94m    labels: Goodbye!\u001b[0;0m\n",
            "\u001b[0;95m     model: Goodbye !\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHey\u001b[0;0m\n",
            "\u001b[1;94m    labels: hi there\u001b[0;0m\n",
            "\u001b[0;95m     model: hi there\u001b[0;0m\n",
            "\u001b[0mDeja vu?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Deja vu!\u001b[0;0m\n",
            "\u001b[0;95m     model: Deja vu !\u001b[0;0m\n",
            "\u001b[0mLast chance\u001b[0;0m\n",
            "\u001b[1;94m    labels: This is it\u001b[0;0m\n",
            "\u001b[0;95m     model: This is it\u001b[0;0m\n",
            "EPOCH DONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hzrDhPS1QCW"
      },
      "source": [
        "Unsurprisingly, we got perfect accuracy. This is because the data set is only a handful of utterances, and we can perfectly memorize it in this LSTM. Nonetheless, a great success!\n",
        "\n",
        "# What's next!\n",
        "\n",
        "The sky's the limit! Be sure to check out our [GitHub](https://github.com/facebookresearch/ParlAI) and [Follow ParlAI on Twitter](https://twitter.com/parlai_parley). We're eager to hear what you are using ParlAI for!\n",
        "\n",
        "Here are some other great resources:\n",
        "- [Our research page](https://parl.ai/projects/)\n",
        "- [ParlAI Documentations](https://parl.ai/docs/index.html)\n",
        "- [Tutorial: Writing a Ranker model](https://parl.ai/docs/tutorial_torch_ranker_agent.html)\n",
        "- [Tutorial: Using Mechanical Turk](https://parl.ai/docs/tutorial_mturk.html)\n",
        "- [Tutorial: Connecting to chat services](https://parl.ai/docs/tutorial_chat_service.html)"
      ]
    }
  ]
}